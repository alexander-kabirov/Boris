{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"semantic_parser_playground_bert_NER.ipynb","provenance":[{"file_id":"1aBIDhLq058lrLjNZk8G-qx_rdLdht9W1","timestamp":1588172891252}],"collapsed_sections":[],"authorship_tag":"ABX9TyMbN4jnsc/cAhn2662pdZpE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ff6af5cd7eec4c1789eaa9e674919920":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_68b96248603f458db40160f3c10ecca4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9ac960862aff4ab4adbbf970267a687e","IPY_MODEL_264d5bf5735d4a6a90ec848a1af3b894"]}},"68b96248603f458db40160f3c10ecca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9ac960862aff4ab4adbbf970267a687e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_027d0074a2bf4bc18a28dc0867d01654","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ae2b34fcc21a4e54be890e4c7b1f5eb2"}},"264d5bf5735d4a6a90ec848a1af3b894":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f41ceb4eb4cc45419a88236fe77aa2a9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 1.16kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b7c3906c955e4f91bafabc9db1307d22"}},"027d0074a2bf4bc18a28dc0867d01654":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ae2b34fcc21a4e54be890e4c7b1f5eb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f41ceb4eb4cc45419a88236fe77aa2a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b7c3906c955e4f91bafabc9db1307d22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"616ba828043f46aba34af8572dcf0605":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_484cf13058cf47a99e7b0f8915e57489","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3f2181c1e45643db9a7904a279aaccf4","IPY_MODEL_cc2bc404f0ed4f3793f0870b72bd6a8a"]}},"484cf13058cf47a99e7b0f8915e57489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f2181c1e45643db9a7904a279aaccf4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_147d2f0672884de197f71c34abbe07fb","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f27486d298b448e9c26dee2971fe37b"}},"cc2bc404f0ed4f3793f0870b72bd6a8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a3859b0ced58439fa6d90ece7b197224","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:09&lt;00:00, 47.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_55b7801e4a1b487d87b96f71d1a6a9d7"}},"147d2f0672884de197f71c34abbe07fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3f27486d298b448e9c26dee2971fe37b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a3859b0ced58439fa6d90ece7b197224":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"55b7801e4a1b487d87b96f71d1a6a9d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8213cfbcd27243679fcf61e54b2921db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8565f5fa7b3b4287ae0b14d856f80348","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_043f8abdbe3e4fed85562f6c3eb60b31","IPY_MODEL_44eee1fe6fa04ee69dd45e0b65f0114e"]}},"8565f5fa7b3b4287ae0b14d856f80348":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"043f8abdbe3e4fed85562f6c3eb60b31":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_46f8572261424fe0b3f63250fc91381d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f239f0ba680c4af4acbb92718126be76"}},"44eee1fe6fa04ee69dd45e0b65f0114e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2b737700ec594d93a38502606bcf2ee9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 613kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f719c55155e14cb4bc73ec2082b0ae28"}},"46f8572261424fe0b3f63250fc91381d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f239f0ba680c4af4acbb92718126be76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b737700ec594d93a38502606bcf2ee9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f719c55155e14cb4bc73ec2082b0ae28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"WYjV2gNAri5G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608977799030,"user_tz":-60,"elapsed":106676,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"01898e74-e25c-4e27-f98a-3e6b5154926c"},"source":["!pip install transformers\n","!pip install torchtext --upgrade"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\r\u001b[K     |▏                               | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 23.3MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 23.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 17.6MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 16.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 14.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 14.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 15.3MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 13.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 13.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112kB 13.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122kB 13.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133kB 13.4MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 153kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 163kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 174kB 13.4MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 194kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 235kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 245kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 256kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 266kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 296kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 307kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 317kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 327kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 337kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 348kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 358kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 368kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 378kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 389kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 399kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 409kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 419kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 440kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 450kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 460kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 471kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 481kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 491kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 501kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 512kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 522kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 532kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 542kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 552kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 563kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 573kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 583kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 593kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 604kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 614kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 624kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 634kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 645kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 655kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 665kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 675kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 686kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 696kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 706kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 716kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 727kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 737kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 747kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 757kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 768kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 778kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 788kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 798kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 808kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 819kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 829kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 839kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 849kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 860kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 870kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 880kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 890kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 901kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 911kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 921kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 931kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 942kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 952kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 962kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 972kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 983kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 993kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.5MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 13.4MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 54.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 53.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=066e17d3e7307fcdf8eaa2daad80dc46824f72997ac0f6433a84134d9b4736b6\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n","Collecting torchtext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/81/be2d72b1ea641afc74557574650a5b421134198de9f68f483ab10d515dca/torchtext-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n","\u001b[K     |████████████████████████████████| 7.0MB 10.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.19.4)\n","Collecting torch==1.7.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/4f/acf48b3a18a8f9223c6616647f0a011a5713a985336088d7c76f3a211374/torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8MB)\n","\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchtext) (0.8)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchtext) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n","Installing collected packages: torch, torchtext\n","  Found existing installation: torch 1.7.0+cu101\n","    Uninstalling torch-1.7.0+cu101:\n","      Successfully uninstalled torch-1.7.0+cu101\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed torch-1.7.1 torchtext-0.8.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vd_LJ_Uok31h","colab":{"base_uri":"https://localhost:8080/","height":217,"referenced_widgets":["ff6af5cd7eec4c1789eaa9e674919920","68b96248603f458db40160f3c10ecca4","9ac960862aff4ab4adbbf970267a687e","264d5bf5735d4a6a90ec848a1af3b894","027d0074a2bf4bc18a28dc0867d01654","ae2b34fcc21a4e54be890e4c7b1f5eb2","f41ceb4eb4cc45419a88236fe77aa2a9","b7c3906c955e4f91bafabc9db1307d22","616ba828043f46aba34af8572dcf0605","484cf13058cf47a99e7b0f8915e57489","3f2181c1e45643db9a7904a279aaccf4","cc2bc404f0ed4f3793f0870b72bd6a8a","147d2f0672884de197f71c34abbe07fb","3f27486d298b448e9c26dee2971fe37b","a3859b0ced58439fa6d90ece7b197224","55b7801e4a1b487d87b96f71d1a6a9d7","8213cfbcd27243679fcf61e54b2921db","8565f5fa7b3b4287ae0b14d856f80348","043f8abdbe3e4fed85562f6c3eb60b31","44eee1fe6fa04ee69dd45e0b65f0114e","46f8572261424fe0b3f63250fc91381d","f239f0ba680c4af4acbb92718126be76","2b737700ec594d93a38502606bcf2ee9","f719c55155e14cb4bc73ec2082b0ae28"]},"executionInfo":{"status":"ok","timestamp":1608977814127,"user_tz":-60,"elapsed":121762,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"8b6939d1-65d9-4f23-8523-03ca88a2b4c7"},"source":["import pandas as pd\n","from random import randint\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","# import BERT \n","from transformers import BertTokenizer, BertModel\n","\n","bert = BertModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n","  return torch._C._cuda_getDeviceCount() > 0\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff6af5cd7eec4c1789eaa9e674919920","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"616ba828043f46aba34af8572dcf0605","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8213cfbcd27243679fcf61e54b2921db","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"70CnZzNFlXZz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608977942150,"user_tz":-60,"elapsed":249778,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"31f899dd-1d0d-4ac2-b173-aa19fcf01598"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OauwIIfhvXyv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608977966864,"user_tz":-60,"elapsed":14196,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"a81f9d8a-ddbb-4b9a-a841-ecf7a1641f86"},"source":["from torchtext.data import Field\n","from torchtext.data import TabularDataset\n","from torchtext.data.utils import get_tokenizer\n","\n","#note we are using the BERT tokenizer for the input\n","tokenize = lambda x: tokenizer.tokenize(x)\n","ner_tokenize = lambda x: x.split(' ')\n","\n","init_token_idx = tokenizer.cls_token_id\n","eos_token_idx = tokenizer.sep_token_id\n","pad_token_idx = tokenizer.pad_token_id\n","unk_token_idx = tokenizer.unk_token_id\n","max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n","\n","TEXT = Field(batch_first = True,\n","             use_vocab = False,\n","             tokenize = tokenize,\n","             preprocessing = tokenizer.convert_tokens_to_ids,\n","             init_token = init_token_idx,\n","             eos_token = eos_token_idx,\n","             pad_token = pad_token_idx,\n","             unk_token = unk_token_idx)\n","             #fix_length = 59) #fix_length=max_input_length\n","\n","# We may use BERT tokenizer or spacy one to split the numbers away\n","REP = Field(sequential=True, tokenize=ner_tokenize,init_token='<sos>',eos_token='<eos>', lower=True) #Why do we need a fixed length here, really? get_tokenizer(\"spacy\")\n","#NER = Field(sequential=True, tokenize=ner_tokenize,init_token='<sos>',eos_token='<eos>', lower=True) #may combine with REP, do you need a fix_length at all? Special tokens? \n","input_fields = [('text',TEXT),('rep',REP),('ner',REP)]\n","train_set, validation_set = TabularDataset.splits(path='/content/drive/My Drive/Datasets/',train='samples_ner_ref.csv',validation='validate_ner_ref.csv',format='csv',skip_header=True,fields=input_fields)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n","/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"KPVZBmY9yqqR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608978046239,"user_tz":-60,"elapsed":874,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"f9ff0d8f-94b5-4378-9780-b0dfa1871c4c"},"source":["text_lens = []\n","rep_lens = []\n","ner_lens = []\n","for example in train_set.examples:\n","  text_lens.append(len(example.text))\n","  rep_lens.append(len(example.rep))\n","  ner_lens.append(len(example.ner))\n","print(np.max(text_lens),np.max(rep_lens),np.max(ner_lens))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["82 27 82\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IRmmMKjyowk9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608978046239,"user_tz":-60,"elapsed":480,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"a87d9d27-1039-4af1-c326-cf43f54046f0"},"source":["text_lens = []\n","rep_lens = []\n","for example in validation_set.examples:\n","  text_lens.append(len(example.text))\n","  rep_lens.append(len(example.rep))\n","print(np.max(text_lens),np.max(rep_lens))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["81 27\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wuRVLqrxu3E6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608978046945,"user_tz":-60,"elapsed":483,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"b768d1d9-4a58-4cb0-dd98-a76b62caea8b"},"source":["for example in train_set.examples:\n","  print(example.text)\n","  print(example.rep)\n","  print(example.ner)\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2425, 2033, 1996, 2922, 11868, 2098]\n","['f_select', 'col_1', 'f_max', 'col_1', 'df_name']\n","['o', 'o', 'o', 'o', 'col_1', 'col_1']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VjVq42y74cjs"},"source":["#TEXT.build_vocab(train_set,validation_set,min_freq = 1) # tried 10, better with 1 - no need to build vocabulary for BERT \n","REP.build_vocab(train_set,validation_set)\n","#NER.build_vocab(train_set,validation_set)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aqjQ0pAedulT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608978050209,"user_tz":-60,"elapsed":606,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"41cae79a-d630-42ac-860c-9a61472214dc"},"source":["REP.vocab.stoi['<eos>']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"BfEnpKdfNhkO"},"source":["from torchtext.data import Iterator, BucketIterator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXpXxsv4NWlQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608978051366,"user_tz":-60,"elapsed":465,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"22c3786c-f42c-4740-dc4e-628584d987fe"},"source":["# this operation does the numerization, <sos>, <eos>, <unk>, <pad> insertion \n","train_iter, val_iter = BucketIterator.splits(\n"," (train_set, validation_set), # we pass in the datasets we want the iterator to draw data from\n"," batch_sizes=(1, 1),device = device) # 16 batch size here for both datasets - LESS IS BETTER (The best one is 1, but super slow )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"dhrYc4Jsvtm1"},"source":["val_iter.sort = False\n","val_iter.sort_within_batch =  False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzSHXVkBRIcP"},"source":["#Previous version \n","def get_batch(batch):\n","    data = batch.text #.transpose(0,1)  #transposing for consistency as BERT preprocessing gives a different dimensionality\n","    #eos_tensor = torch.tensor([batch.rep,REP.vocab.stoi['<eos>']]*batch.rep.size()[1],device=device)\n","    #sos_tensor = torch.tensor([batch.rep,REP.vocab.stoi['<sos>']]*batch.rep.size()[1],device=device)\n","    target_parse = batch.rep\n","    target_ner = batch.ner\n","    #target_concat = torch.cat((batch.rep,batch.ner)) #torch.cat((batch.rep,REP.vocab.stoi['<eos>']),dim=1) # + REP.vocab.stoi['<sos>'] + batch.ner\n","    return data, target_parse, target_ner"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7dX7yuxCRRj1","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1592163364745,"user_tz":-120,"elapsed":16369,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"c98b9721-4aa2-4fb9-9f7b-89a7e87a8a2a"},"source":["for batch in train_iter:\n","  data,target_parse,target_ner = get_batch(batch)\n","  print(data.size(),target_parse.size(),target_ner.size())\n","  break\n","#  encoding = bert(data)[0]\n","#  linear = nn.Linear(768, len(REP.vocab.stoi)).to(device) #encoding.size()[1]\n","#  print(linear(encoding).size())\n","  #print(target)\n","  #pad_id = REP.vocab.stoi['<pad>']\n","  #print(torch.tensor(target == pad_id,device=device).permute(1,0))\n","# break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 20]) torch.Size([13, 1]) torch.Size([20, 1])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MeCfAB6TRJ5O"},"source":[""]},{"cell_type":"code","metadata":{"id":"_ihMtQYOqQ5F"},"source":["class TransformerModel(nn.Module):\n","\n","    def __init__(self,nreptoken, nhead, nhid, nlayers, dropout=0.5): #nnertoken\n","        super(TransformerModel, self).__init__()\n","        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer,Transformer\n","        self.model_type = 'Transformer'\n","        \n","        embedding_dim = bert.config.to_dict()['hidden_size']\n","        self.bert = bert\n","        self.src_mask = None\n","        self.tgt_mask = None\n","        self.pos_encoder = PositionalEncoding(embedding_dim, dropout)\n","        #encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        #decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n","        ##self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = nn.Embedding(nreptoken, embedding_dim) #encode to the same dimensionality as bert \n","        self.bert_encoder = bert\n","        self.transformer = Transformer(d_model=embedding_dim, nhead=nhead, num_encoder_layers=nlayers, num_decoder_layers=nlayers, dim_feedforward=nhid, dropout=dropout, activation='relu', custom_encoder=None, custom_decoder=None)\n","        self.linear = nn.Linear(embedding_dim, nreptoken)\n","        self.linear_ner = nn.Linear(embedding_dim, nreptoken)\n","        #self.decoder = TransformerDecoder(decoder_layers, nlayers)\n","        #self.init_weights()\n","\n","    def _generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src,tgt):\n","        #if self.src_mask is None or self.src_mask.size(0) != len(src):\n","        #    device = src.device\n","        #    mask = self._generate_square_subsequent_mask(len(src)).to(device)\n","        #   self.src_mask = mask\n","        if self.tgt_mask is None or self.tgt_mask.size(0) != len(tgt):\n","          device = tgt.device\n","          mask = self._generate_square_subsequent_mask(len(tgt)).to(device)\n","          self.tgt_mask = mask\n","\n","        #get the padding masks\n","        pad_id = REP.vocab.stoi['<pad>'] \n","        src_pad_mask = torch.tensor(src == pad_token_idx)\n","        tgt_pad_mask = torch.tensor(tgt == pad_id).permute(1,0)\n","\n","        #print(src.size())\n","        #with torch.no_grad(): #try to enable for NER purposes \n","        src =  self.bert_encoder(src)[0]\n","\n","        src = src.permute(1,0,2) # you need to permute the bert output because it has flipped batch/seq dimensions, otherwise transformer doesn't comsume it correctly \n","        src = self.pos_encoder(src)\n","        #print(src.size())\n","\n","        #tgt = tgt.transpose(0,1)\n","        #with torch.no_grad():\n","        tgt =  self.encoder(tgt)\n","        tgt = self.pos_encoder(tgt)\n","\n","        #output = self.transformer_encoder(src, self.src_mask)\n","        #print(src.size(),tgt.size())\n","        \n","        #,src_key_padding_mask = src_pad_mask, tgt_key_padding_mask = tgt_pad_mask\n","        output = self.transformer(src = src,tgt = tgt, tgt_mask = self.tgt_mask,src_key_padding_mask = src_pad_mask, tgt_key_padding_mask = tgt_pad_mask) #you need to transpose the bert's output\n","        output = self.linear(output)\n","        output_ner = self.linear_ner(src) #should we add a encoder layer here?, probably no\n","        output = torch.cat((output,output_ner))\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V27yt8YIsWKQ"},"source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJICRTuib2_U"},"source":["nreptokens = len(REP.vocab.stoi) #REP\n","#nnertokens = len(NER.vocab.stoi) #NER\n","#emsize = 300 # embedding dimension\n","nhid = 300 # the dimension of the feedforward network model in nn.TransformerEncoder\n","nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n","nhead = 4 # the number of heads in the multiheadattention models\n","dropout = 0.1 # the dropout value\n","model = TransformerModel(nreptokens, nhead, nhid, nlayers, dropout).to(device) #nnertokens,"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xvN4LlZ_36V"},"source":["#for name, param in model.named_parameters():                \n","#    if name.startswith('bert'):\n","#        param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qny9C8zB_61M","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1592163365072,"user_tz":-120,"elapsed":16684,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"0c8b3bfc-36ed-43c6-ec0d-9e233593e43c"},"source":["# disabling the training of bert\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 125,589,226 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0BNcL4F9cH3z"},"source":["criterion = nn.CrossEntropyLoss() # weight=weights, reduction = 'sum', idea - calciulate the weights based on frequency - less frequent get more weight\n","lr = 0.5 # learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n","\n","import time\n","def train():\n","    model.train() # Turn on the train mode\n","    total_loss = 0.\n","    start_time = time.time()\n","    nreptokens = len(REP.vocab.stoi)\n","    for i,batch in enumerate(train_iter):\n","        data, targets_parse,targets_ner = get_batch(batch)\n","        optimizer.zero_grad()\n","        output = model(data,targets_parse[:-1, :])\n","        loss = criterion(output.view(-1, nreptokens), torch.cat((targets_parse[1:, :],targets_ner)).contiguous().view(-1))\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","        #print(output.size())\n","        total_loss += loss.item()\n","        log_interval = 50\n","        if i % log_interval == 0 and i > 0:\n","            cur_loss = total_loss / log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n","                  'lr {:02.2f} | ms/batch {:5.2f} | '\n","                  'loss {:5.2f} | ppl {:8.2f}'.format(\n","                    epoch, i, len(train_set) // 16, scheduler.get_lr()[0], #\n","                    elapsed * 1000 / log_interval,\n","                    cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","\n","def evaluate(eval_model, data_source):\n","    eval_model.eval() # Turn on the evaluation mode\n","    total_loss = 0.\n","    ntokens = len(REP.vocab.stoi)\n","    with torch.no_grad():\n","        for batch in data_source:\n","            data, targets_parse,targets_ner = get_batch(batch)\n","            output = eval_model(data,targets_parse[:-1, :])\n","            output_flat = output.view(-1, ntokens)\n","            total_loss += len(data) * criterion(output_flat, torch.cat((targets_parse[1:, :],targets_ner)).contiguous().view(-1)).item()\n","    return total_loss / (len(data_source) - 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CMl67Q-HcXKM","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592164965379,"user_tz":-120,"elapsed":1616984,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"d3abe36b-f4d1-49af-b5f4-81f566cf523a"},"source":["best_val_loss = float(\"inf\")\n","epochs = 1 #18 in case of 1 batch reaches 0\n","best_model = None\n","\n","for epoch in range(1, epochs + 1):\n","    epoch_start_time = time.time()\n","    train()\n","    val_loss = evaluate(model, val_iter)\n","    print('-' * 89)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                     val_loss, math.exp(val_loss)))\n","    print('-' * 89)\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_model = model\n","\n","    scheduler.step()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  \"please use `get_last_lr()`.\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["| epoch   1 |    50/ 2126 batches | lr 0.50 | ms/batch 59.35 | loss  1.39 | ppl     4.03\n","| epoch   1 |   100/ 2126 batches | lr 0.50 | ms/batch 48.98 | loss  0.60 | ppl     1.82\n","| epoch   1 |   150/ 2126 batches | lr 0.50 | ms/batch 48.07 | loss  0.42 | ppl     1.52\n","| epoch   1 |   200/ 2126 batches | lr 0.50 | ms/batch 48.77 | loss  0.43 | ppl     1.53\n","| epoch   1 |   250/ 2126 batches | lr 0.50 | ms/batch 48.72 | loss  0.39 | ppl     1.48\n","| epoch   1 |   300/ 2126 batches | lr 0.50 | ms/batch 48.74 | loss  0.33 | ppl     1.38\n","| epoch   1 |   350/ 2126 batches | lr 0.50 | ms/batch 48.50 | loss  0.35 | ppl     1.43\n","| epoch   1 |   400/ 2126 batches | lr 0.50 | ms/batch 48.77 | loss  0.30 | ppl     1.34\n","| epoch   1 |   450/ 2126 batches | lr 0.50 | ms/batch 49.27 | loss  0.27 | ppl     1.31\n","| epoch   1 |   500/ 2126 batches | lr 0.50 | ms/batch 48.32 | loss  0.22 | ppl     1.25\n","| epoch   1 |   550/ 2126 batches | lr 0.50 | ms/batch 48.76 | loss  0.24 | ppl     1.27\n","| epoch   1 |   600/ 2126 batches | lr 0.50 | ms/batch 48.74 | loss  0.24 | ppl     1.27\n","| epoch   1 |   650/ 2126 batches | lr 0.50 | ms/batch 48.26 | loss  0.21 | ppl     1.24\n","| epoch   1 |   700/ 2126 batches | lr 0.50 | ms/batch 48.69 | loss  0.19 | ppl     1.20\n","| epoch   1 |   750/ 2126 batches | lr 0.50 | ms/batch 48.03 | loss  0.22 | ppl     1.25\n","| epoch   1 |   800/ 2126 batches | lr 0.50 | ms/batch 48.18 | loss  0.17 | ppl     1.19\n","| epoch   1 |   850/ 2126 batches | lr 0.50 | ms/batch 48.72 | loss  0.20 | ppl     1.22\n","| epoch   1 |   900/ 2126 batches | lr 0.50 | ms/batch 48.73 | loss  0.09 | ppl     1.10\n","| epoch   1 |   950/ 2126 batches | lr 0.50 | ms/batch 48.52 | loss  0.15 | ppl     1.16\n","| epoch   1 |  1000/ 2126 batches | lr 0.50 | ms/batch 48.00 | loss  0.09 | ppl     1.10\n","| epoch   1 |  1050/ 2126 batches | lr 0.50 | ms/batch 48.32 | loss  0.13 | ppl     1.13\n","| epoch   1 |  1100/ 2126 batches | lr 0.50 | ms/batch 47.79 | loss  0.15 | ppl     1.16\n","| epoch   1 |  1150/ 2126 batches | lr 0.50 | ms/batch 46.91 | loss  0.09 | ppl     1.10\n","| epoch   1 |  1200/ 2126 batches | lr 0.50 | ms/batch 46.79 | loss  0.04 | ppl     1.05\n","| epoch   1 |  1250/ 2126 batches | lr 0.50 | ms/batch 48.51 | loss  0.16 | ppl     1.17\n","| epoch   1 |  1300/ 2126 batches | lr 0.50 | ms/batch 47.60 | loss  0.09 | ppl     1.10\n","| epoch   1 |  1350/ 2126 batches | lr 0.50 | ms/batch 46.82 | loss  0.10 | ppl     1.10\n","| epoch   1 |  1400/ 2126 batches | lr 0.50 | ms/batch 46.59 | loss  0.06 | ppl     1.06\n","| epoch   1 |  1450/ 2126 batches | lr 0.50 | ms/batch 47.31 | loss  0.10 | ppl     1.10\n","| epoch   1 |  1500/ 2126 batches | lr 0.50 | ms/batch 46.81 | loss  0.10 | ppl     1.10\n","| epoch   1 |  1550/ 2126 batches | lr 0.50 | ms/batch 46.48 | loss  0.04 | ppl     1.04\n","| epoch   1 |  1600/ 2126 batches | lr 0.50 | ms/batch 46.94 | loss  0.06 | ppl     1.06\n","| epoch   1 |  1650/ 2126 batches | lr 0.50 | ms/batch 46.87 | loss  0.07 | ppl     1.07\n","| epoch   1 |  1700/ 2126 batches | lr 0.50 | ms/batch 47.10 | loss  0.04 | ppl     1.04\n","| epoch   1 |  1750/ 2126 batches | lr 0.50 | ms/batch 46.74 | loss  0.05 | ppl     1.05\n","| epoch   1 |  1800/ 2126 batches | lr 0.50 | ms/batch 46.84 | loss  0.06 | ppl     1.06\n","| epoch   1 |  1850/ 2126 batches | lr 0.50 | ms/batch 47.59 | loss  0.07 | ppl     1.07\n","| epoch   1 |  1900/ 2126 batches | lr 0.50 | ms/batch 46.93 | loss  0.08 | ppl     1.08\n","| epoch   1 |  1950/ 2126 batches | lr 0.50 | ms/batch 47.41 | loss  0.08 | ppl     1.08\n","| epoch   1 |  2000/ 2126 batches | lr 0.50 | ms/batch 47.25 | loss  0.09 | ppl     1.10\n","| epoch   1 |  2050/ 2126 batches | lr 0.50 | ms/batch 46.76 | loss  0.05 | ppl     1.05\n","| epoch   1 |  2100/ 2126 batches | lr 0.50 | ms/batch 47.46 | loss  0.06 | ppl     1.06\n","| epoch   1 |  2150/ 2126 batches | lr 0.50 | ms/batch 47.06 | loss  0.07 | ppl     1.08\n","| epoch   1 |  2200/ 2126 batches | lr 0.50 | ms/batch 47.17 | loss  0.05 | ppl     1.05\n","| epoch   1 |  2250/ 2126 batches | lr 0.50 | ms/batch 47.10 | loss  0.08 | ppl     1.08\n","| epoch   1 |  2300/ 2126 batches | lr 0.50 | ms/batch 47.24 | loss  0.06 | ppl     1.06\n","| epoch   1 |  2350/ 2126 batches | lr 0.50 | ms/batch 46.18 | loss  0.04 | ppl     1.04\n","| epoch   1 |  2400/ 2126 batches | lr 0.50 | ms/batch 48.21 | loss  0.05 | ppl     1.05\n","| epoch   1 |  2450/ 2126 batches | lr 0.50 | ms/batch 49.08 | loss  0.04 | ppl     1.04\n","| epoch   1 |  2500/ 2126 batches | lr 0.50 | ms/batch 49.62 | loss  0.03 | ppl     1.04\n","| epoch   1 |  2550/ 2126 batches | lr 0.50 | ms/batch 47.70 | loss  0.05 | ppl     1.05\n","| epoch   1 |  2600/ 2126 batches | lr 0.50 | ms/batch 49.19 | loss  0.03 | ppl     1.03\n","| epoch   1 |  2650/ 2126 batches | lr 0.50 | ms/batch 48.45 | loss  0.04 | ppl     1.04\n","| epoch   1 |  2700/ 2126 batches | lr 0.50 | ms/batch 47.99 | loss  0.05 | ppl     1.05\n","| epoch   1 |  2750/ 2126 batches | lr 0.50 | ms/batch 48.17 | loss  0.04 | ppl     1.04\n","| epoch   1 |  2800/ 2126 batches | lr 0.50 | ms/batch 48.57 | loss  0.05 | ppl     1.05\n","| epoch   1 |  2850/ 2126 batches | lr 0.50 | ms/batch 46.35 | loss  0.04 | ppl     1.04\n","| epoch   1 |  2900/ 2126 batches | lr 0.50 | ms/batch 47.04 | loss  0.04 | ppl     1.04\n","| epoch   1 |  2950/ 2126 batches | lr 0.50 | ms/batch 47.04 | loss  0.05 | ppl     1.06\n","| epoch   1 |  3000/ 2126 batches | lr 0.50 | ms/batch 47.28 | loss  0.05 | ppl     1.05\n","| epoch   1 |  3050/ 2126 batches | lr 0.50 | ms/batch 48.06 | loss  0.05 | ppl     1.05\n","| epoch   1 |  3100/ 2126 batches | lr 0.50 | ms/batch 46.67 | loss  0.05 | ppl     1.05\n","| epoch   1 |  3150/ 2126 batches | lr 0.50 | ms/batch 47.23 | loss  0.07 | ppl     1.07\n","| epoch   1 |  3200/ 2126 batches | lr 0.50 | ms/batch 46.86 | loss  0.12 | ppl     1.12\n","| epoch   1 |  3250/ 2126 batches | lr 0.50 | ms/batch 46.91 | loss  0.03 | ppl     1.04\n","| epoch   1 |  3300/ 2126 batches | lr 0.50 | ms/batch 46.39 | loss  0.06 | ppl     1.06\n","| epoch   1 |  3350/ 2126 batches | lr 0.50 | ms/batch 46.75 | loss  0.05 | ppl     1.05\n","| epoch   1 |  3400/ 2126 batches | lr 0.50 | ms/batch 46.27 | loss  0.05 | ppl     1.05\n","| epoch   1 |  3450/ 2126 batches | lr 0.50 | ms/batch 46.63 | loss  0.04 | ppl     1.04\n","| epoch   1 |  3500/ 2126 batches | lr 0.50 | ms/batch 47.45 | loss  0.07 | ppl     1.07\n","| epoch   1 |  3550/ 2126 batches | lr 0.50 | ms/batch 46.48 | loss  0.03 | ppl     1.03\n","| epoch   1 |  3600/ 2126 batches | lr 0.50 | ms/batch 46.84 | loss  0.09 | ppl     1.09\n","| epoch   1 |  3650/ 2126 batches | lr 0.50 | ms/batch 46.69 | loss  0.03 | ppl     1.03\n","| epoch   1 |  3700/ 2126 batches | lr 0.50 | ms/batch 46.14 | loss  0.03 | ppl     1.03\n","| epoch   1 |  3750/ 2126 batches | lr 0.50 | ms/batch 46.90 | loss  0.03 | ppl     1.03\n","| epoch   1 |  3800/ 2126 batches | lr 0.50 | ms/batch 47.04 | loss  0.04 | ppl     1.04\n","| epoch   1 |  3850/ 2126 batches | lr 0.50 | ms/batch 47.05 | loss  0.07 | ppl     1.07\n","| epoch   1 |  3900/ 2126 batches | lr 0.50 | ms/batch 46.21 | loss  0.05 | ppl     1.06\n","| epoch   1 |  3950/ 2126 batches | lr 0.50 | ms/batch 46.96 | loss  0.03 | ppl     1.03\n","| epoch   1 |  4000/ 2126 batches | lr 0.50 | ms/batch 47.74 | loss  0.06 | ppl     1.06\n","| epoch   1 |  4050/ 2126 batches | lr 0.50 | ms/batch 47.22 | loss  0.03 | ppl     1.03\n","| epoch   1 |  4100/ 2126 batches | lr 0.50 | ms/batch 47.44 | loss  0.04 | ppl     1.04\n","| epoch   1 |  4150/ 2126 batches | lr 0.50 | ms/batch 46.84 | loss  0.04 | ppl     1.05\n","| epoch   1 |  4200/ 2126 batches | lr 0.50 | ms/batch 46.31 | loss  0.04 | ppl     1.04\n","| epoch   1 |  4250/ 2126 batches | lr 0.50 | ms/batch 46.91 | loss  0.04 | ppl     1.04\n","| epoch   1 |  4300/ 2126 batches | lr 0.50 | ms/batch 47.64 | loss  0.05 | ppl     1.06\n","| epoch   1 |  4350/ 2126 batches | lr 0.50 | ms/batch 47.62 | loss  0.04 | ppl     1.04\n","| epoch   1 |  4400/ 2126 batches | lr 0.50 | ms/batch 47.36 | loss  0.02 | ppl     1.02\n","| epoch   1 |  4450/ 2126 batches | lr 0.50 | ms/batch 47.15 | loss  0.03 | ppl     1.03\n","| epoch   1 |  4500/ 2126 batches | lr 0.50 | ms/batch 47.21 | loss  0.05 | ppl     1.05\n","| epoch   1 |  4550/ 2126 batches | lr 0.50 | ms/batch 46.18 | loss  0.02 | ppl     1.02\n","| epoch   1 |  4600/ 2126 batches | lr 0.50 | ms/batch 47.79 | loss  0.03 | ppl     1.03\n","| epoch   1 |  4650/ 2126 batches | lr 0.50 | ms/batch 47.00 | loss  0.04 | ppl     1.04\n","| epoch   1 |  4700/ 2126 batches | lr 0.50 | ms/batch 46.51 | loss  0.10 | ppl     1.11\n","| epoch   1 |  4750/ 2126 batches | lr 0.50 | ms/batch 46.34 | loss  0.03 | ppl     1.03\n","| epoch   1 |  4800/ 2126 batches | lr 0.50 | ms/batch 46.63 | loss  0.03 | ppl     1.03\n","| epoch   1 |  4850/ 2126 batches | lr 0.50 | ms/batch 47.15 | loss  0.05 | ppl     1.05\n","| epoch   1 |  4900/ 2126 batches | lr 0.50 | ms/batch 46.13 | loss  0.04 | ppl     1.04\n","| epoch   1 |  4950/ 2126 batches | lr 0.50 | ms/batch 46.83 | loss  0.03 | ppl     1.03\n","| epoch   1 |  5000/ 2126 batches | lr 0.50 | ms/batch 46.66 | loss  0.04 | ppl     1.04\n","| epoch   1 |  5050/ 2126 batches | lr 0.50 | ms/batch 47.17 | loss  0.02 | ppl     1.02\n","| epoch   1 |  5100/ 2126 batches | lr 0.50 | ms/batch 46.47 | loss  0.04 | ppl     1.04\n","| epoch   1 |  5150/ 2126 batches | lr 0.50 | ms/batch 47.65 | loss  0.06 | ppl     1.06\n","| epoch   1 |  5200/ 2126 batches | lr 0.50 | ms/batch 46.37 | loss  0.03 | ppl     1.03\n","| epoch   1 |  5250/ 2126 batches | lr 0.50 | ms/batch 46.21 | loss  0.03 | ppl     1.03\n","| epoch   1 |  5300/ 2126 batches | lr 0.50 | ms/batch 46.86 | loss  0.03 | ppl     1.03\n","| epoch   1 |  5350/ 2126 batches | lr 0.50 | ms/batch 46.83 | loss  0.02 | ppl     1.02\n","| epoch   1 |  5400/ 2126 batches | lr 0.50 | ms/batch 46.68 | loss  0.04 | ppl     1.04\n","| epoch   1 |  5450/ 2126 batches | lr 0.50 | ms/batch 46.74 | loss  0.03 | ppl     1.03\n","| epoch   1 |  5500/ 2126 batches | lr 0.50 | ms/batch 46.85 | loss  0.03 | ppl     1.03\n","| epoch   1 |  5550/ 2126 batches | lr 0.50 | ms/batch 46.40 | loss  0.03 | ppl     1.03\n","| epoch   1 |  5600/ 2126 batches | lr 0.50 | ms/batch 46.17 | loss  0.02 | ppl     1.02\n","| epoch   1 |  5650/ 2126 batches | lr 0.50 | ms/batch 46.49 | loss  0.02 | ppl     1.02\n","| epoch   1 |  5700/ 2126 batches | lr 0.50 | ms/batch 47.43 | loss  0.04 | ppl     1.05\n","| epoch   1 |  5750/ 2126 batches | lr 0.50 | ms/batch 46.48 | loss  0.03 | ppl     1.03\n","| epoch   1 |  5800/ 2126 batches | lr 0.50 | ms/batch 46.11 | loss  0.03 | ppl     1.03\n","| epoch   1 |  5850/ 2126 batches | lr 0.50 | ms/batch 46.61 | loss  0.07 | ppl     1.08\n","| epoch   1 |  5900/ 2126 batches | lr 0.50 | ms/batch 45.97 | loss  0.03 | ppl     1.03\n","| epoch   1 |  5950/ 2126 batches | lr 0.50 | ms/batch 47.17 | loss  0.03 | ppl     1.03\n","| epoch   1 |  6000/ 2126 batches | lr 0.50 | ms/batch 46.83 | loss  0.03 | ppl     1.03\n","| epoch   1 |  6050/ 2126 batches | lr 0.50 | ms/batch 47.55 | loss  0.07 | ppl     1.07\n","| epoch   1 |  6100/ 2126 batches | lr 0.50 | ms/batch 46.18 | loss  0.02 | ppl     1.02\n","| epoch   1 |  6150/ 2126 batches | lr 0.50 | ms/batch 46.90 | loss  0.02 | ppl     1.02\n","| epoch   1 |  6200/ 2126 batches | lr 0.50 | ms/batch 47.51 | loss  0.03 | ppl     1.03\n","| epoch   1 |  6250/ 2126 batches | lr 0.50 | ms/batch 45.91 | loss  0.03 | ppl     1.03\n","| epoch   1 |  6300/ 2126 batches | lr 0.50 | ms/batch 46.15 | loss  0.02 | ppl     1.02\n","| epoch   1 |  6350/ 2126 batches | lr 0.50 | ms/batch 46.47 | loss  0.02 | ppl     1.02\n","| epoch   1 |  6400/ 2126 batches | lr 0.50 | ms/batch 46.11 | loss  0.03 | ppl     1.03\n","| epoch   1 |  6450/ 2126 batches | lr 0.50 | ms/batch 47.14 | loss  0.04 | ppl     1.04\n","| epoch   1 |  6500/ 2126 batches | lr 0.50 | ms/batch 47.42 | loss  0.04 | ppl     1.04\n","| epoch   1 |  6550/ 2126 batches | lr 0.50 | ms/batch 46.77 | loss  0.02 | ppl     1.02\n","| epoch   1 |  6600/ 2126 batches | lr 0.50 | ms/batch 46.54 | loss  0.04 | ppl     1.04\n","| epoch   1 |  6650/ 2126 batches | lr 0.50 | ms/batch 46.14 | loss  0.03 | ppl     1.03\n","| epoch   1 |  6700/ 2126 batches | lr 0.50 | ms/batch 46.09 | loss  0.02 | ppl     1.02\n","| epoch   1 |  6750/ 2126 batches | lr 0.50 | ms/batch 46.68 | loss  0.08 | ppl     1.09\n","| epoch   1 |  6800/ 2126 batches | lr 0.50 | ms/batch 46.43 | loss  0.05 | ppl     1.05\n","| epoch   1 |  6850/ 2126 batches | lr 0.50 | ms/batch 46.64 | loss  0.03 | ppl     1.03\n","| epoch   1 |  6900/ 2126 batches | lr 0.50 | ms/batch 46.08 | loss  0.03 | ppl     1.03\n","| epoch   1 |  6950/ 2126 batches | lr 0.50 | ms/batch 46.32 | loss  0.07 | ppl     1.07\n","| epoch   1 |  7000/ 2126 batches | lr 0.50 | ms/batch 46.20 | loss  0.03 | ppl     1.03\n","| epoch   1 |  7050/ 2126 batches | lr 0.50 | ms/batch 46.02 | loss  0.03 | ppl     1.03\n","| epoch   1 |  7100/ 2126 batches | lr 0.50 | ms/batch 46.26 | loss  0.03 | ppl     1.03\n","| epoch   1 |  7150/ 2126 batches | lr 0.50 | ms/batch 46.58 | loss  0.04 | ppl     1.04\n","| epoch   1 |  7200/ 2126 batches | lr 0.50 | ms/batch 45.98 | loss  0.03 | ppl     1.03\n","| epoch   1 |  7250/ 2126 batches | lr 0.50 | ms/batch 46.48 | loss  0.05 | ppl     1.05\n","| epoch   1 |  7300/ 2126 batches | lr 0.50 | ms/batch 46.09 | loss  0.03 | ppl     1.03\n","| epoch   1 |  7350/ 2126 batches | lr 0.50 | ms/batch 46.10 | loss  0.04 | ppl     1.04\n","| epoch   1 |  7400/ 2126 batches | lr 0.50 | ms/batch 46.12 | loss  0.02 | ppl     1.02\n","| epoch   1 |  7450/ 2126 batches | lr 0.50 | ms/batch 46.12 | loss  0.04 | ppl     1.04\n","| epoch   1 |  7500/ 2126 batches | lr 0.50 | ms/batch 46.57 | loss  0.04 | ppl     1.04\n","| epoch   1 |  7550/ 2126 batches | lr 0.50 | ms/batch 46.20 | loss  0.04 | ppl     1.04\n","| epoch   1 |  7600/ 2126 batches | lr 0.50 | ms/batch 46.12 | loss  0.03 | ppl     1.04\n","| epoch   1 |  7650/ 2126 batches | lr 0.50 | ms/batch 47.77 | loss  0.08 | ppl     1.09\n","| epoch   1 |  7700/ 2126 batches | lr 0.50 | ms/batch 46.81 | loss  0.06 | ppl     1.06\n","| epoch   1 |  7750/ 2126 batches | lr 0.50 | ms/batch 47.35 | loss  0.03 | ppl     1.03\n","| epoch   1 |  7800/ 2126 batches | lr 0.50 | ms/batch 47.72 | loss  0.04 | ppl     1.05\n","| epoch   1 |  7850/ 2126 batches | lr 0.50 | ms/batch 46.72 | loss  0.02 | ppl     1.02\n","| epoch   1 |  7900/ 2126 batches | lr 0.50 | ms/batch 46.91 | loss  0.08 | ppl     1.08\n","| epoch   1 |  7950/ 2126 batches | lr 0.50 | ms/batch 47.02 | loss  0.04 | ppl     1.04\n","| epoch   1 |  8000/ 2126 batches | lr 0.50 | ms/batch 46.45 | loss  0.04 | ppl     1.04\n","| epoch   1 |  8050/ 2126 batches | lr 0.50 | ms/batch 46.18 | loss  0.10 | ppl     1.11\n","| epoch   1 |  8100/ 2126 batches | lr 0.50 | ms/batch 45.80 | loss  0.02 | ppl     1.02\n","| epoch   1 |  8150/ 2126 batches | lr 0.50 | ms/batch 46.20 | loss  0.04 | ppl     1.04\n","| epoch   1 |  8200/ 2126 batches | lr 0.50 | ms/batch 45.75 | loss  0.02 | ppl     1.02\n","| epoch   1 |  8250/ 2126 batches | lr 0.50 | ms/batch 45.85 | loss  0.02 | ppl     1.02\n","| epoch   1 |  8300/ 2126 batches | lr 0.50 | ms/batch 45.34 | loss  0.05 | ppl     1.05\n","| epoch   1 |  8350/ 2126 batches | lr 0.50 | ms/batch 46.19 | loss  0.03 | ppl     1.03\n","| epoch   1 |  8400/ 2126 batches | lr 0.50 | ms/batch 45.71 | loss  0.02 | ppl     1.02\n","| epoch   1 |  8450/ 2126 batches | lr 0.50 | ms/batch 45.30 | loss  0.04 | ppl     1.04\n","| epoch   1 |  8500/ 2126 batches | lr 0.50 | ms/batch 45.50 | loss  0.03 | ppl     1.03\n","| epoch   1 |  8550/ 2126 batches | lr 0.50 | ms/batch 45.52 | loss  0.04 | ppl     1.04\n","| epoch   1 |  8600/ 2126 batches | lr 0.50 | ms/batch 45.39 | loss  0.02 | ppl     1.02\n","| epoch   1 |  8650/ 2126 batches | lr 0.50 | ms/batch 45.84 | loss  0.02 | ppl     1.02\n","| epoch   1 |  8700/ 2126 batches | lr 0.50 | ms/batch 46.15 | loss  0.04 | ppl     1.04\n","| epoch   1 |  8750/ 2126 batches | lr 0.50 | ms/batch 46.29 | loss  0.04 | ppl     1.04\n","| epoch   1 |  8800/ 2126 batches | lr 0.50 | ms/batch 45.91 | loss  0.04 | ppl     1.04\n","| epoch   1 |  8850/ 2126 batches | lr 0.50 | ms/batch 45.62 | loss  0.02 | ppl     1.02\n","| epoch   1 |  8900/ 2126 batches | lr 0.50 | ms/batch 46.19 | loss  0.04 | ppl     1.04\n","| epoch   1 |  8950/ 2126 batches | lr 0.50 | ms/batch 46.11 | loss  0.04 | ppl     1.04\n","| epoch   1 |  9000/ 2126 batches | lr 0.50 | ms/batch 45.45 | loss  0.03 | ppl     1.04\n","| epoch   1 |  9050/ 2126 batches | lr 0.50 | ms/batch 45.17 | loss  0.04 | ppl     1.05\n","| epoch   1 |  9100/ 2126 batches | lr 0.50 | ms/batch 46.34 | loss  0.03 | ppl     1.03\n","| epoch   1 |  9150/ 2126 batches | lr 0.50 | ms/batch 45.82 | loss  0.04 | ppl     1.04\n","| epoch   1 |  9200/ 2126 batches | lr 0.50 | ms/batch 45.84 | loss  0.04 | ppl     1.04\n","| epoch   1 |  9250/ 2126 batches | lr 0.50 | ms/batch 46.52 | loss  0.02 | ppl     1.02\n","| epoch   1 |  9300/ 2126 batches | lr 0.50 | ms/batch 46.35 | loss  0.02 | ppl     1.02\n","| epoch   1 |  9350/ 2126 batches | lr 0.50 | ms/batch 47.84 | loss  0.03 | ppl     1.03\n","| epoch   1 |  9400/ 2126 batches | lr 0.50 | ms/batch 47.54 | loss  0.03 | ppl     1.03\n","| epoch   1 |  9450/ 2126 batches | lr 0.50 | ms/batch 46.64 | loss  0.04 | ppl     1.04\n","| epoch   1 |  9500/ 2126 batches | lr 0.50 | ms/batch 46.41 | loss  0.03 | ppl     1.03\n","| epoch   1 |  9550/ 2126 batches | lr 0.50 | ms/batch 46.05 | loss  0.03 | ppl     1.03\n","| epoch   1 |  9600/ 2126 batches | lr 0.50 | ms/batch 46.37 | loss  0.02 | ppl     1.02\n","| epoch   1 |  9650/ 2126 batches | lr 0.50 | ms/batch 46.27 | loss  0.04 | ppl     1.04\n","| epoch   1 |  9700/ 2126 batches | lr 0.50 | ms/batch 45.91 | loss  0.06 | ppl     1.06\n","| epoch   1 |  9750/ 2126 batches | lr 0.50 | ms/batch 46.06 | loss  0.03 | ppl     1.03\n","| epoch   1 |  9800/ 2126 batches | lr 0.50 | ms/batch 45.55 | loss  0.04 | ppl     1.04\n","| epoch   1 |  9850/ 2126 batches | lr 0.50 | ms/batch 45.12 | loss  0.02 | ppl     1.02\n","| epoch   1 |  9900/ 2126 batches | lr 0.50 | ms/batch 45.49 | loss  0.04 | ppl     1.04\n","| epoch   1 |  9950/ 2126 batches | lr 0.50 | ms/batch 46.19 | loss  0.04 | ppl     1.04\n","| epoch   1 | 10000/ 2126 batches | lr 0.50 | ms/batch 45.93 | loss  0.03 | ppl     1.03\n","| epoch   1 | 10050/ 2126 batches | lr 0.50 | ms/batch 45.67 | loss  0.04 | ppl     1.04\n","| epoch   1 | 10100/ 2126 batches | lr 0.50 | ms/batch 46.40 | loss  0.02 | ppl     1.02\n","| epoch   1 | 10150/ 2126 batches | lr 0.50 | ms/batch 45.38 | loss  0.03 | ppl     1.03\n","| epoch   1 | 10200/ 2126 batches | lr 0.50 | ms/batch 45.20 | loss  0.03 | ppl     1.03\n","| epoch   1 | 10250/ 2126 batches | lr 0.50 | ms/batch 45.86 | loss  0.03 | ppl     1.03\n","| epoch   1 | 10300/ 2126 batches | lr 0.50 | ms/batch 46.00 | loss  0.04 | ppl     1.04\n","| epoch   1 | 10350/ 2126 batches | lr 0.50 | ms/batch 46.27 | loss  0.04 | ppl     1.04\n","| epoch   1 | 10400/ 2126 batches | lr 0.50 | ms/batch 46.04 | loss  0.02 | ppl     1.02\n","| epoch   1 | 10450/ 2126 batches | lr 0.50 | ms/batch 45.90 | loss  0.04 | ppl     1.04\n","| epoch   1 | 10500/ 2126 batches | lr 0.50 | ms/batch 45.69 | loss  0.04 | ppl     1.04\n","| epoch   1 | 10550/ 2126 batches | lr 0.50 | ms/batch 46.20 | loss  0.06 | ppl     1.06\n","| epoch   1 | 10600/ 2126 batches | lr 0.50 | ms/batch 46.92 | loss  0.03 | ppl     1.03\n","| epoch   1 | 10650/ 2126 batches | lr 0.50 | ms/batch 46.54 | loss  0.03 | ppl     1.03\n","| epoch   1 | 10700/ 2126 batches | lr 0.50 | ms/batch 46.27 | loss  0.03 | ppl     1.03\n","| epoch   1 | 10750/ 2126 batches | lr 0.50 | ms/batch 45.46 | loss  0.04 | ppl     1.04\n","| epoch   1 | 10800/ 2126 batches | lr 0.50 | ms/batch 45.57 | loss  0.02 | ppl     1.02\n","| epoch   1 | 10850/ 2126 batches | lr 0.50 | ms/batch 45.69 | loss  0.05 | ppl     1.06\n","| epoch   1 | 10900/ 2126 batches | lr 0.50 | ms/batch 46.04 | loss  0.03 | ppl     1.03\n","| epoch   1 | 10950/ 2126 batches | lr 0.50 | ms/batch 45.60 | loss  0.03 | ppl     1.03\n","| epoch   1 | 11000/ 2126 batches | lr 0.50 | ms/batch 45.69 | loss  0.04 | ppl     1.04\n","| epoch   1 | 11050/ 2126 batches | lr 0.50 | ms/batch 45.93 | loss  0.02 | ppl     1.02\n","| epoch   1 | 11100/ 2126 batches | lr 0.50 | ms/batch 46.51 | loss  0.03 | ppl     1.03\n","| epoch   1 | 11150/ 2126 batches | lr 0.50 | ms/batch 45.66 | loss  0.02 | ppl     1.02\n","| epoch   1 | 11200/ 2126 batches | lr 0.50 | ms/batch 45.94 | loss  0.02 | ppl     1.02\n","| epoch   1 | 11250/ 2126 batches | lr 0.50 | ms/batch 46.75 | loss  0.03 | ppl     1.03\n","| epoch   1 | 11300/ 2126 batches | lr 0.50 | ms/batch 45.68 | loss  0.03 | ppl     1.03\n","| epoch   1 | 11350/ 2126 batches | lr 0.50 | ms/batch 47.14 | loss  0.03 | ppl     1.03\n","| epoch   1 | 11400/ 2126 batches | lr 0.50 | ms/batch 46.71 | loss  0.02 | ppl     1.02\n","| epoch   1 | 11450/ 2126 batches | lr 0.50 | ms/batch 46.14 | loss  0.04 | ppl     1.04\n","| epoch   1 | 11500/ 2126 batches | lr 0.50 | ms/batch 45.36 | loss  0.02 | ppl     1.03\n","| epoch   1 | 11550/ 2126 batches | lr 0.50 | ms/batch 46.24 | loss  0.02 | ppl     1.02\n","| epoch   1 | 11600/ 2126 batches | lr 0.50 | ms/batch 46.79 | loss  0.02 | ppl     1.02\n","| epoch   1 | 11650/ 2126 batches | lr 0.50 | ms/batch 46.00 | loss  0.04 | ppl     1.04\n","| epoch   1 | 11700/ 2126 batches | lr 0.50 | ms/batch 45.66 | loss  0.02 | ppl     1.02\n","| epoch   1 | 11750/ 2126 batches | lr 0.50 | ms/batch 46.44 | loss  0.03 | ppl     1.03\n","| epoch   1 | 11800/ 2126 batches | lr 0.50 | ms/batch 45.19 | loss  0.02 | ppl     1.02\n","| epoch   1 | 11850/ 2126 batches | lr 0.50 | ms/batch 45.52 | loss  0.02 | ppl     1.02\n","| epoch   1 | 11900/ 2126 batches | lr 0.50 | ms/batch 45.76 | loss  0.02 | ppl     1.02\n","| epoch   1 | 11950/ 2126 batches | lr 0.50 | ms/batch 45.10 | loss  0.02 | ppl     1.02\n","| epoch   1 | 12000/ 2126 batches | lr 0.50 | ms/batch 45.80 | loss  0.01 | ppl     1.01\n","| epoch   1 | 12050/ 2126 batches | lr 0.50 | ms/batch 45.69 | loss  0.02 | ppl     1.02\n","| epoch   1 | 12100/ 2126 batches | lr 0.50 | ms/batch 45.61 | loss  0.04 | ppl     1.04\n","| epoch   1 | 12150/ 2126 batches | lr 0.50 | ms/batch 45.58 | loss  0.01 | ppl     1.01\n","| epoch   1 | 12200/ 2126 batches | lr 0.50 | ms/batch 44.97 | loss  0.01 | ppl     1.01\n","| epoch   1 | 12250/ 2126 batches | lr 0.50 | ms/batch 45.05 | loss  0.01 | ppl     1.01\n","| epoch   1 | 12300/ 2126 batches | lr 0.50 | ms/batch 45.23 | loss  0.02 | ppl     1.02\n","| epoch   1 | 12350/ 2126 batches | lr 0.50 | ms/batch 45.08 | loss  0.02 | ppl     1.02\n","| epoch   1 | 12400/ 2126 batches | lr 0.50 | ms/batch 45.56 | loss  0.02 | ppl     1.02\n","| epoch   1 | 12450/ 2126 batches | lr 0.50 | ms/batch 45.07 | loss  0.01 | ppl     1.02\n","| epoch   1 | 12500/ 2126 batches | lr 0.50 | ms/batch 44.87 | loss  0.01 | ppl     1.01\n","| epoch   1 | 12550/ 2126 batches | lr 0.50 | ms/batch 45.21 | loss  0.06 | ppl     1.06\n","| epoch   1 | 12600/ 2126 batches | lr 0.50 | ms/batch 44.97 | loss  0.01 | ppl     1.01\n","| epoch   1 | 12650/ 2126 batches | lr 0.50 | ms/batch 44.71 | loss  0.02 | ppl     1.02\n","| epoch   1 | 12700/ 2126 batches | lr 0.50 | ms/batch 45.31 | loss  0.02 | ppl     1.02\n","| epoch   1 | 12750/ 2126 batches | lr 0.50 | ms/batch 45.26 | loss  0.02 | ppl     1.02\n","| epoch   1 | 12800/ 2126 batches | lr 0.50 | ms/batch 45.19 | loss  0.01 | ppl     1.01\n","| epoch   1 | 12850/ 2126 batches | lr 0.50 | ms/batch 44.99 | loss  0.01 | ppl     1.01\n","| epoch   1 | 12900/ 2126 batches | lr 0.50 | ms/batch 45.47 | loss  0.02 | ppl     1.02\n","| epoch   1 | 12950/ 2126 batches | lr 0.50 | ms/batch 45.09 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13000/ 2126 batches | lr 0.50 | ms/batch 45.48 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13050/ 2126 batches | lr 0.50 | ms/batch 45.70 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13100/ 2126 batches | lr 0.50 | ms/batch 45.28 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13150/ 2126 batches | lr 0.50 | ms/batch 45.55 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13200/ 2126 batches | lr 0.50 | ms/batch 45.71 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13250/ 2126 batches | lr 0.50 | ms/batch 45.51 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13300/ 2126 batches | lr 0.50 | ms/batch 45.44 | loss  0.02 | ppl     1.02\n","| epoch   1 | 13350/ 2126 batches | lr 0.50 | ms/batch 44.93 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13400/ 2126 batches | lr 0.50 | ms/batch 45.45 | loss  0.02 | ppl     1.02\n","| epoch   1 | 13450/ 2126 batches | lr 0.50 | ms/batch 45.21 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13500/ 2126 batches | lr 0.50 | ms/batch 45.62 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13550/ 2126 batches | lr 0.50 | ms/batch 45.07 | loss  0.02 | ppl     1.02\n","| epoch   1 | 13600/ 2126 batches | lr 0.50 | ms/batch 45.47 | loss  0.02 | ppl     1.02\n","| epoch   1 | 13650/ 2126 batches | lr 0.50 | ms/batch 45.66 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13700/ 2126 batches | lr 0.50 | ms/batch 45.41 | loss  0.03 | ppl     1.03\n","| epoch   1 | 13750/ 2126 batches | lr 0.50 | ms/batch 45.92 | loss  0.02 | ppl     1.02\n","| epoch   1 | 13800/ 2126 batches | lr 0.50 | ms/batch 45.17 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13850/ 2126 batches | lr 0.50 | ms/batch 45.68 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13900/ 2126 batches | lr 0.50 | ms/batch 44.87 | loss  0.01 | ppl     1.01\n","| epoch   1 | 13950/ 2126 batches | lr 0.50 | ms/batch 45.74 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14000/ 2126 batches | lr 0.50 | ms/batch 45.15 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14050/ 2126 batches | lr 0.50 | ms/batch 45.46 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14100/ 2126 batches | lr 0.50 | ms/batch 45.32 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14150/ 2126 batches | lr 0.50 | ms/batch 45.16 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14200/ 2126 batches | lr 0.50 | ms/batch 45.45 | loss  0.03 | ppl     1.03\n","| epoch   1 | 14250/ 2126 batches | lr 0.50 | ms/batch 45.25 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14300/ 2126 batches | lr 0.50 | ms/batch 45.20 | loss  0.02 | ppl     1.02\n","| epoch   1 | 14350/ 2126 batches | lr 0.50 | ms/batch 45.35 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14400/ 2126 batches | lr 0.50 | ms/batch 45.99 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14450/ 2126 batches | lr 0.50 | ms/batch 45.14 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14500/ 2126 batches | lr 0.50 | ms/batch 45.14 | loss  0.02 | ppl     1.02\n","| epoch   1 | 14550/ 2126 batches | lr 0.50 | ms/batch 45.33 | loss  0.02 | ppl     1.02\n","| epoch   1 | 14600/ 2126 batches | lr 0.50 | ms/batch 45.19 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14650/ 2126 batches | lr 0.50 | ms/batch 45.52 | loss  0.02 | ppl     1.02\n","| epoch   1 | 14700/ 2126 batches | lr 0.50 | ms/batch 45.45 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14750/ 2126 batches | lr 0.50 | ms/batch 45.88 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14800/ 2126 batches | lr 0.50 | ms/batch 45.23 | loss  0.02 | ppl     1.02\n","| epoch   1 | 14850/ 2126 batches | lr 0.50 | ms/batch 45.77 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14900/ 2126 batches | lr 0.50 | ms/batch 45.89 | loss  0.01 | ppl     1.01\n","| epoch   1 | 14950/ 2126 batches | lr 0.50 | ms/batch 46.24 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15000/ 2126 batches | lr 0.50 | ms/batch 45.77 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15050/ 2126 batches | lr 0.50 | ms/batch 45.92 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15100/ 2126 batches | lr 0.50 | ms/batch 45.59 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15150/ 2126 batches | lr 0.50 | ms/batch 44.80 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15200/ 2126 batches | lr 0.50 | ms/batch 44.86 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15250/ 2126 batches | lr 0.50 | ms/batch 44.56 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15300/ 2126 batches | lr 0.50 | ms/batch 45.36 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15350/ 2126 batches | lr 0.50 | ms/batch 45.68 | loss  0.02 | ppl     1.02\n","| epoch   1 | 15400/ 2126 batches | lr 0.50 | ms/batch 45.52 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15450/ 2126 batches | lr 0.50 | ms/batch 45.40 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15500/ 2126 batches | lr 0.50 | ms/batch 45.92 | loss  0.12 | ppl     1.12\n","| epoch   1 | 15550/ 2126 batches | lr 0.50 | ms/batch 45.12 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15600/ 2126 batches | lr 0.50 | ms/batch 45.54 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15650/ 2126 batches | lr 0.50 | ms/batch 45.56 | loss  0.02 | ppl     1.02\n","| epoch   1 | 15700/ 2126 batches | lr 0.50 | ms/batch 44.83 | loss  0.02 | ppl     1.02\n","| epoch   1 | 15750/ 2126 batches | lr 0.50 | ms/batch 45.24 | loss  0.02 | ppl     1.02\n","| epoch   1 | 15800/ 2126 batches | lr 0.50 | ms/batch 45.27 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15850/ 2126 batches | lr 0.50 | ms/batch 44.56 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15900/ 2126 batches | lr 0.50 | ms/batch 44.82 | loss  0.01 | ppl     1.01\n","| epoch   1 | 15950/ 2126 batches | lr 0.50 | ms/batch 44.64 | loss  0.03 | ppl     1.03\n","| epoch   1 | 16000/ 2126 batches | lr 0.50 | ms/batch 44.70 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16050/ 2126 batches | lr 0.50 | ms/batch 45.60 | loss  0.02 | ppl     1.02\n","| epoch   1 | 16100/ 2126 batches | lr 0.50 | ms/batch 46.82 | loss  0.02 | ppl     1.02\n","| epoch   1 | 16150/ 2126 batches | lr 0.50 | ms/batch 44.89 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16200/ 2126 batches | lr 0.50 | ms/batch 45.50 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16250/ 2126 batches | lr 0.50 | ms/batch 45.84 | loss  0.02 | ppl     1.02\n","| epoch   1 | 16300/ 2126 batches | lr 0.50 | ms/batch 44.87 | loss  0.02 | ppl     1.02\n","| epoch   1 | 16350/ 2126 batches | lr 0.50 | ms/batch 44.78 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16400/ 2126 batches | lr 0.50 | ms/batch 45.11 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16450/ 2126 batches | lr 0.50 | ms/batch 44.88 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16500/ 2126 batches | lr 0.50 | ms/batch 44.77 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16550/ 2126 batches | lr 0.50 | ms/batch 44.90 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16600/ 2126 batches | lr 0.50 | ms/batch 45.17 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16650/ 2126 batches | lr 0.50 | ms/batch 45.11 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16700/ 2126 batches | lr 0.50 | ms/batch 45.45 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16750/ 2126 batches | lr 0.50 | ms/batch 45.45 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16800/ 2126 batches | lr 0.50 | ms/batch 45.25 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16850/ 2126 batches | lr 0.50 | ms/batch 44.96 | loss  0.02 | ppl     1.02\n","| epoch   1 | 16900/ 2126 batches | lr 0.50 | ms/batch 45.09 | loss  0.01 | ppl     1.01\n","| epoch   1 | 16950/ 2126 batches | lr 0.50 | ms/batch 44.99 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17000/ 2126 batches | lr 0.50 | ms/batch 44.69 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17050/ 2126 batches | lr 0.50 | ms/batch 45.28 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17100/ 2126 batches | lr 0.50 | ms/batch 44.74 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17150/ 2126 batches | lr 0.50 | ms/batch 45.26 | loss  0.02 | ppl     1.02\n","| epoch   1 | 17200/ 2126 batches | lr 0.50 | ms/batch 44.68 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17250/ 2126 batches | lr 0.50 | ms/batch 44.75 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17300/ 2126 batches | lr 0.50 | ms/batch 46.10 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17350/ 2126 batches | lr 0.50 | ms/batch 45.17 | loss  0.02 | ppl     1.02\n","| epoch   1 | 17400/ 2126 batches | lr 0.50 | ms/batch 45.91 | loss  0.02 | ppl     1.02\n","| epoch   1 | 17450/ 2126 batches | lr 0.50 | ms/batch 45.23 | loss  0.02 | ppl     1.02\n","| epoch   1 | 17500/ 2126 batches | lr 0.50 | ms/batch 44.90 | loss  0.02 | ppl     1.02\n","| epoch   1 | 17550/ 2126 batches | lr 0.50 | ms/batch 45.36 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17600/ 2126 batches | lr 0.50 | ms/batch 44.45 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17650/ 2126 batches | lr 0.50 | ms/batch 45.66 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17700/ 2126 batches | lr 0.50 | ms/batch 44.98 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17750/ 2126 batches | lr 0.50 | ms/batch 45.01 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17800/ 2126 batches | lr 0.50 | ms/batch 44.76 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17850/ 2126 batches | lr 0.50 | ms/batch 45.06 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17900/ 2126 batches | lr 0.50 | ms/batch 44.79 | loss  0.01 | ppl     1.01\n","| epoch   1 | 17950/ 2126 batches | lr 0.50 | ms/batch 45.23 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18000/ 2126 batches | lr 0.50 | ms/batch 45.20 | loss  0.07 | ppl     1.07\n","| epoch   1 | 18050/ 2126 batches | lr 0.50 | ms/batch 44.73 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18100/ 2126 batches | lr 0.50 | ms/batch 45.07 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18150/ 2126 batches | lr 0.50 | ms/batch 44.93 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18200/ 2126 batches | lr 0.50 | ms/batch 45.24 | loss  0.03 | ppl     1.03\n","| epoch   1 | 18250/ 2126 batches | lr 0.50 | ms/batch 45.51 | loss  0.06 | ppl     1.07\n","| epoch   1 | 18300/ 2126 batches | lr 0.50 | ms/batch 44.75 | loss  0.02 | ppl     1.02\n","| epoch   1 | 18350/ 2126 batches | lr 0.50 | ms/batch 45.36 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18400/ 2126 batches | lr 0.50 | ms/batch 45.48 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18450/ 2126 batches | lr 0.50 | ms/batch 45.42 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18500/ 2126 batches | lr 0.50 | ms/batch 45.57 | loss  0.02 | ppl     1.02\n","| epoch   1 | 18550/ 2126 batches | lr 0.50 | ms/batch 45.58 | loss  0.02 | ppl     1.02\n","| epoch   1 | 18600/ 2126 batches | lr 0.50 | ms/batch 45.59 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18650/ 2126 batches | lr 0.50 | ms/batch 45.21 | loss  0.02 | ppl     1.02\n","| epoch   1 | 18700/ 2126 batches | lr 0.50 | ms/batch 44.67 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18750/ 2126 batches | lr 0.50 | ms/batch 44.99 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18800/ 2126 batches | lr 0.50 | ms/batch 45.47 | loss  0.02 | ppl     1.02\n","| epoch   1 | 18850/ 2126 batches | lr 0.50 | ms/batch 44.86 | loss  0.02 | ppl     1.02\n","| epoch   1 | 18900/ 2126 batches | lr 0.50 | ms/batch 44.50 | loss  0.01 | ppl     1.01\n","| epoch   1 | 18950/ 2126 batches | lr 0.50 | ms/batch 44.33 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19000/ 2126 batches | lr 0.50 | ms/batch 45.04 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19050/ 2126 batches | lr 0.50 | ms/batch 45.49 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19100/ 2126 batches | lr 0.50 | ms/batch 45.19 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19150/ 2126 batches | lr 0.50 | ms/batch 45.09 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19200/ 2126 batches | lr 0.50 | ms/batch 45.17 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19250/ 2126 batches | lr 0.50 | ms/batch 44.66 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19300/ 2126 batches | lr 0.50 | ms/batch 45.24 | loss  0.02 | ppl     1.02\n","| epoch   1 | 19350/ 2126 batches | lr 0.50 | ms/batch 44.86 | loss  0.02 | ppl     1.02\n","| epoch   1 | 19400/ 2126 batches | lr 0.50 | ms/batch 44.91 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19450/ 2126 batches | lr 0.50 | ms/batch 45.44 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19500/ 2126 batches | lr 0.50 | ms/batch 44.83 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19550/ 2126 batches | lr 0.50 | ms/batch 44.79 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19600/ 2126 batches | lr 0.50 | ms/batch 45.13 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19650/ 2126 batches | lr 0.50 | ms/batch 45.64 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19700/ 2126 batches | lr 0.50 | ms/batch 45.11 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19750/ 2126 batches | lr 0.50 | ms/batch 44.91 | loss  0.02 | ppl     1.02\n","| epoch   1 | 19800/ 2126 batches | lr 0.50 | ms/batch 45.18 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19850/ 2126 batches | lr 0.50 | ms/batch 45.01 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19900/ 2126 batches | lr 0.50 | ms/batch 44.82 | loss  0.01 | ppl     1.01\n","| epoch   1 | 19950/ 2126 batches | lr 0.50 | ms/batch 45.03 | loss  0.04 | ppl     1.04\n","| epoch   1 | 20000/ 2126 batches | lr 0.50 | ms/batch 44.72 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20050/ 2126 batches | lr 0.50 | ms/batch 44.69 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20100/ 2126 batches | lr 0.50 | ms/batch 44.89 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20150/ 2126 batches | lr 0.50 | ms/batch 45.87 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20200/ 2126 batches | lr 0.50 | ms/batch 45.79 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20250/ 2126 batches | lr 0.50 | ms/batch 45.28 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20300/ 2126 batches | lr 0.50 | ms/batch 45.41 | loss  0.02 | ppl     1.02\n","| epoch   1 | 20350/ 2126 batches | lr 0.50 | ms/batch 45.54 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20400/ 2126 batches | lr 0.50 | ms/batch 45.03 | loss  0.02 | ppl     1.02\n","| epoch   1 | 20450/ 2126 batches | lr 0.50 | ms/batch 44.85 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20500/ 2126 batches | lr 0.50 | ms/batch 44.55 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20550/ 2126 batches | lr 0.50 | ms/batch 44.73 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20600/ 2126 batches | lr 0.50 | ms/batch 45.15 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20650/ 2126 batches | lr 0.50 | ms/batch 44.69 | loss  0.02 | ppl     1.02\n","| epoch   1 | 20700/ 2126 batches | lr 0.50 | ms/batch 44.78 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20750/ 2126 batches | lr 0.50 | ms/batch 44.85 | loss  0.02 | ppl     1.02\n","| epoch   1 | 20800/ 2126 batches | lr 0.50 | ms/batch 46.16 | loss  0.01 | ppl     1.01\n","| epoch   1 | 20850/ 2126 batches | lr 0.50 | ms/batch 45.27 | loss  0.03 | ppl     1.03\n","| epoch   1 | 20900/ 2126 batches | lr 0.50 | ms/batch 44.72 | loss  0.02 | ppl     1.02\n","| epoch   1 | 20950/ 2126 batches | lr 0.50 | ms/batch 44.56 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21000/ 2126 batches | lr 0.50 | ms/batch 44.82 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21050/ 2126 batches | lr 0.50 | ms/batch 45.18 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21100/ 2126 batches | lr 0.50 | ms/batch 45.47 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21150/ 2126 batches | lr 0.50 | ms/batch 45.62 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21200/ 2126 batches | lr 0.50 | ms/batch 45.00 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21250/ 2126 batches | lr 0.50 | ms/batch 44.49 | loss  0.02 | ppl     1.02\n","| epoch   1 | 21300/ 2126 batches | lr 0.50 | ms/batch 44.93 | loss  0.02 | ppl     1.02\n","| epoch   1 | 21350/ 2126 batches | lr 0.50 | ms/batch 45.09 | loss  0.02 | ppl     1.02\n","| epoch   1 | 21400/ 2126 batches | lr 0.50 | ms/batch 44.76 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21450/ 2126 batches | lr 0.50 | ms/batch 44.62 | loss  0.02 | ppl     1.02\n","| epoch   1 | 21500/ 2126 batches | lr 0.50 | ms/batch 44.92 | loss  0.02 | ppl     1.02\n","| epoch   1 | 21550/ 2126 batches | lr 0.50 | ms/batch 44.65 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21600/ 2126 batches | lr 0.50 | ms/batch 44.29 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21650/ 2126 batches | lr 0.50 | ms/batch 45.19 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21700/ 2126 batches | lr 0.50 | ms/batch 45.10 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21750/ 2126 batches | lr 0.50 | ms/batch 44.59 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21800/ 2126 batches | lr 0.50 | ms/batch 45.03 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21850/ 2126 batches | lr 0.50 | ms/batch 44.70 | loss  0.01 | ppl     1.01\n","| epoch   1 | 21900/ 2126 batches | lr 0.50 | ms/batch 45.38 | loss  0.02 | ppl     1.02\n","| epoch   1 | 21950/ 2126 batches | lr 0.50 | ms/batch 45.35 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22000/ 2126 batches | lr 0.50 | ms/batch 45.52 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22050/ 2126 batches | lr 0.50 | ms/batch 45.37 | loss  0.02 | ppl     1.02\n","| epoch   1 | 22100/ 2126 batches | lr 0.50 | ms/batch 45.25 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22150/ 2126 batches | lr 0.50 | ms/batch 44.89 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22200/ 2126 batches | lr 0.50 | ms/batch 44.86 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22250/ 2126 batches | lr 0.50 | ms/batch 45.03 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22300/ 2126 batches | lr 0.50 | ms/batch 45.13 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22350/ 2126 batches | lr 0.50 | ms/batch 44.99 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22400/ 2126 batches | lr 0.50 | ms/batch 44.92 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22450/ 2126 batches | lr 0.50 | ms/batch 44.80 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22500/ 2126 batches | lr 0.50 | ms/batch 44.79 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22550/ 2126 batches | lr 0.50 | ms/batch 45.73 | loss  0.02 | ppl     1.02\n","| epoch   1 | 22600/ 2126 batches | lr 0.50 | ms/batch 45.39 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22650/ 2126 batches | lr 0.50 | ms/batch 44.99 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22700/ 2126 batches | lr 0.50 | ms/batch 45.62 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22750/ 2126 batches | lr 0.50 | ms/batch 45.21 | loss  0.02 | ppl     1.02\n","| epoch   1 | 22800/ 2126 batches | lr 0.50 | ms/batch 44.67 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22850/ 2126 batches | lr 0.50 | ms/batch 44.73 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22900/ 2126 batches | lr 0.50 | ms/batch 44.66 | loss  0.01 | ppl     1.01\n","| epoch   1 | 22950/ 2126 batches | lr 0.50 | ms/batch 46.13 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23000/ 2126 batches | lr 0.50 | ms/batch 45.33 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23050/ 2126 batches | lr 0.50 | ms/batch 46.42 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23100/ 2126 batches | lr 0.50 | ms/batch 45.95 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23150/ 2126 batches | lr 0.50 | ms/batch 45.73 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23200/ 2126 batches | lr 0.50 | ms/batch 44.50 | loss  0.02 | ppl     1.02\n","| epoch   1 | 23250/ 2126 batches | lr 0.50 | ms/batch 46.62 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23300/ 2126 batches | lr 0.50 | ms/batch 45.77 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23350/ 2126 batches | lr 0.50 | ms/batch 44.69 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23400/ 2126 batches | lr 0.50 | ms/batch 45.28 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23450/ 2126 batches | lr 0.50 | ms/batch 44.40 | loss  0.02 | ppl     1.02\n","| epoch   1 | 23500/ 2126 batches | lr 0.50 | ms/batch 45.31 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23550/ 2126 batches | lr 0.50 | ms/batch 45.02 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23600/ 2126 batches | lr 0.50 | ms/batch 44.53 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23650/ 2126 batches | lr 0.50 | ms/batch 44.58 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23700/ 2126 batches | lr 0.50 | ms/batch 45.68 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23750/ 2126 batches | lr 0.50 | ms/batch 46.16 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23800/ 2126 batches | lr 0.50 | ms/batch 46.66 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23850/ 2126 batches | lr 0.50 | ms/batch 45.78 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23900/ 2126 batches | lr 0.50 | ms/batch 45.47 | loss  0.01 | ppl     1.01\n","| epoch   1 | 23950/ 2126 batches | lr 0.50 | ms/batch 45.32 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24000/ 2126 batches | lr 0.50 | ms/batch 44.99 | loss  0.01 | ppl     1.02\n","| epoch   1 | 24050/ 2126 batches | lr 0.50 | ms/batch 46.09 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24100/ 2126 batches | lr 0.50 | ms/batch 46.18 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24150/ 2126 batches | lr 0.50 | ms/batch 45.64 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24200/ 2126 batches | lr 0.50 | ms/batch 45.68 | loss  0.02 | ppl     1.02\n","| epoch   1 | 24250/ 2126 batches | lr 0.50 | ms/batch 45.56 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24300/ 2126 batches | lr 0.50 | ms/batch 45.36 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24350/ 2126 batches | lr 0.50 | ms/batch 45.38 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24400/ 2126 batches | lr 0.50 | ms/batch 44.72 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24450/ 2126 batches | lr 0.50 | ms/batch 44.57 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24500/ 2126 batches | lr 0.50 | ms/batch 45.47 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24550/ 2126 batches | lr 0.50 | ms/batch 44.54 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24600/ 2126 batches | lr 0.50 | ms/batch 44.55 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24650/ 2126 batches | lr 0.50 | ms/batch 44.85 | loss  0.02 | ppl     1.02\n","| epoch   1 | 24700/ 2126 batches | lr 0.50 | ms/batch 44.55 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24750/ 2126 batches | lr 0.50 | ms/batch 45.07 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24800/ 2126 batches | lr 0.50 | ms/batch 45.05 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24850/ 2126 batches | lr 0.50 | ms/batch 45.06 | loss  0.01 | ppl     1.01\n","| epoch   1 | 24900/ 2126 batches | lr 0.50 | ms/batch 45.05 | loss  0.02 | ppl     1.02\n","| epoch   1 | 24950/ 2126 batches | lr 0.50 | ms/batch 45.50 | loss  0.02 | ppl     1.02\n","| epoch   1 | 25000/ 2126 batches | lr 0.50 | ms/batch 45.18 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25050/ 2126 batches | lr 0.50 | ms/batch 44.39 | loss  0.02 | ppl     1.02\n","| epoch   1 | 25100/ 2126 batches | lr 0.50 | ms/batch 45.77 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25150/ 2126 batches | lr 0.50 | ms/batch 44.88 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25200/ 2126 batches | lr 0.50 | ms/batch 45.64 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25250/ 2126 batches | lr 0.50 | ms/batch 45.63 | loss  0.02 | ppl     1.02\n","| epoch   1 | 25300/ 2126 batches | lr 0.50 | ms/batch 44.51 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25350/ 2126 batches | lr 0.50 | ms/batch 44.80 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25400/ 2126 batches | lr 0.50 | ms/batch 45.49 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25450/ 2126 batches | lr 0.50 | ms/batch 45.13 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25500/ 2126 batches | lr 0.50 | ms/batch 45.47 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25550/ 2126 batches | lr 0.50 | ms/batch 45.26 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25600/ 2126 batches | lr 0.50 | ms/batch 45.53 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25650/ 2126 batches | lr 0.50 | ms/batch 45.05 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25700/ 2126 batches | lr 0.50 | ms/batch 45.87 | loss  0.02 | ppl     1.02\n","| epoch   1 | 25750/ 2126 batches | lr 0.50 | ms/batch 44.55 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25800/ 2126 batches | lr 0.50 | ms/batch 44.79 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25850/ 2126 batches | lr 0.50 | ms/batch 44.58 | loss  0.01 | ppl     1.01\n","| epoch   1 | 25900/ 2126 batches | lr 0.50 | ms/batch 44.69 | loss  0.02 | ppl     1.02\n","| epoch   1 | 25950/ 2126 batches | lr 0.50 | ms/batch 44.55 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26000/ 2126 batches | lr 0.50 | ms/batch 44.85 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26050/ 2126 batches | lr 0.50 | ms/batch 45.06 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26100/ 2126 batches | lr 0.50 | ms/batch 44.64 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26150/ 2126 batches | lr 0.50 | ms/batch 45.14 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26200/ 2126 batches | lr 0.50 | ms/batch 45.32 | loss  0.02 | ppl     1.02\n","| epoch   1 | 26250/ 2126 batches | lr 0.50 | ms/batch 44.92 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26300/ 2126 batches | lr 0.50 | ms/batch 44.91 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26350/ 2126 batches | lr 0.50 | ms/batch 44.91 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26400/ 2126 batches | lr 0.50 | ms/batch 45.22 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26450/ 2126 batches | lr 0.50 | ms/batch 45.29 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26500/ 2126 batches | lr 0.50 | ms/batch 44.94 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26550/ 2126 batches | lr 0.50 | ms/batch 44.77 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26600/ 2126 batches | lr 0.50 | ms/batch 45.43 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26650/ 2126 batches | lr 0.50 | ms/batch 44.59 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26700/ 2126 batches | lr 0.50 | ms/batch 44.52 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26750/ 2126 batches | lr 0.50 | ms/batch 45.09 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26800/ 2126 batches | lr 0.50 | ms/batch 44.50 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26850/ 2126 batches | lr 0.50 | ms/batch 44.62 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26900/ 2126 batches | lr 0.50 | ms/batch 45.23 | loss  0.01 | ppl     1.01\n","| epoch   1 | 26950/ 2126 batches | lr 0.50 | ms/batch 44.66 | loss  0.02 | ppl     1.02\n","| epoch   1 | 27000/ 2126 batches | lr 0.50 | ms/batch 44.81 | loss  0.02 | ppl     1.02\n","| epoch   1 | 27050/ 2126 batches | lr 0.50 | ms/batch 44.59 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27100/ 2126 batches | lr 0.50 | ms/batch 45.08 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27150/ 2126 batches | lr 0.50 | ms/batch 45.23 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27200/ 2126 batches | lr 0.50 | ms/batch 46.11 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27250/ 2126 batches | lr 0.50 | ms/batch 45.20 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27300/ 2126 batches | lr 0.50 | ms/batch 45.74 | loss  0.06 | ppl     1.06\n","| epoch   1 | 27350/ 2126 batches | lr 0.50 | ms/batch 45.68 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27400/ 2126 batches | lr 0.50 | ms/batch 46.00 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27450/ 2126 batches | lr 0.50 | ms/batch 45.74 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27500/ 2126 batches | lr 0.50 | ms/batch 45.95 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27550/ 2126 batches | lr 0.50 | ms/batch 45.13 | loss  0.02 | ppl     1.02\n","| epoch   1 | 27600/ 2126 batches | lr 0.50 | ms/batch 44.72 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27650/ 2126 batches | lr 0.50 | ms/batch 44.95 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27700/ 2126 batches | lr 0.50 | ms/batch 45.07 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27750/ 2126 batches | lr 0.50 | ms/batch 45.68 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27800/ 2126 batches | lr 0.50 | ms/batch 45.80 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27850/ 2126 batches | lr 0.50 | ms/batch 45.23 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27900/ 2126 batches | lr 0.50 | ms/batch 45.54 | loss  0.01 | ppl     1.01\n","| epoch   1 | 27950/ 2126 batches | lr 0.50 | ms/batch 44.93 | loss  0.02 | ppl     1.02\n","| epoch   1 | 28000/ 2126 batches | lr 0.50 | ms/batch 44.88 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28050/ 2126 batches | lr 0.50 | ms/batch 45.22 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28100/ 2126 batches | lr 0.50 | ms/batch 44.90 | loss  0.02 | ppl     1.02\n","| epoch   1 | 28150/ 2126 batches | lr 0.50 | ms/batch 45.27 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28200/ 2126 batches | lr 0.50 | ms/batch 45.70 | loss  0.02 | ppl     1.02\n","| epoch   1 | 28250/ 2126 batches | lr 0.50 | ms/batch 44.95 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28300/ 2126 batches | lr 0.50 | ms/batch 44.91 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28350/ 2126 batches | lr 0.50 | ms/batch 45.93 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28400/ 2126 batches | lr 0.50 | ms/batch 44.64 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28450/ 2126 batches | lr 0.50 | ms/batch 45.18 | loss  0.02 | ppl     1.02\n","| epoch   1 | 28500/ 2126 batches | lr 0.50 | ms/batch 45.20 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28550/ 2126 batches | lr 0.50 | ms/batch 44.78 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28600/ 2126 batches | lr 0.50 | ms/batch 44.56 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28650/ 2126 batches | lr 0.50 | ms/batch 45.02 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28700/ 2126 batches | lr 0.50 | ms/batch 44.51 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28750/ 2126 batches | lr 0.50 | ms/batch 45.21 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28800/ 2126 batches | lr 0.50 | ms/batch 44.85 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28850/ 2126 batches | lr 0.50 | ms/batch 45.22 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28900/ 2126 batches | lr 0.50 | ms/batch 46.20 | loss  0.01 | ppl     1.01\n","| epoch   1 | 28950/ 2126 batches | lr 0.50 | ms/batch 44.53 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29000/ 2126 batches | lr 0.50 | ms/batch 44.98 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29050/ 2126 batches | lr 0.50 | ms/batch 45.07 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29100/ 2126 batches | lr 0.50 | ms/batch 45.40 | loss  0.02 | ppl     1.02\n","| epoch   1 | 29150/ 2126 batches | lr 0.50 | ms/batch 45.49 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29200/ 2126 batches | lr 0.50 | ms/batch 45.65 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29250/ 2126 batches | lr 0.50 | ms/batch 45.34 | loss  0.02 | ppl     1.02\n","| epoch   1 | 29300/ 2126 batches | lr 0.50 | ms/batch 45.49 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29350/ 2126 batches | lr 0.50 | ms/batch 44.73 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29400/ 2126 batches | lr 0.50 | ms/batch 45.41 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29450/ 2126 batches | lr 0.50 | ms/batch 45.12 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29500/ 2126 batches | lr 0.50 | ms/batch 45.55 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29550/ 2126 batches | lr 0.50 | ms/batch 44.71 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29600/ 2126 batches | lr 0.50 | ms/batch 44.81 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29650/ 2126 batches | lr 0.50 | ms/batch 44.56 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29700/ 2126 batches | lr 0.50 | ms/batch 46.00 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29750/ 2126 batches | lr 0.50 | ms/batch 45.06 | loss  0.01 | ppl     1.02\n","| epoch   1 | 29800/ 2126 batches | lr 0.50 | ms/batch 45.52 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29850/ 2126 batches | lr 0.50 | ms/batch 45.52 | loss  0.01 | ppl     1.01\n","| epoch   1 | 29900/ 2126 batches | lr 0.50 | ms/batch 45.49 | loss  0.02 | ppl     1.02\n","| epoch   1 | 29950/ 2126 batches | lr 0.50 | ms/batch 45.89 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30000/ 2126 batches | lr 0.50 | ms/batch 46.00 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30050/ 2126 batches | lr 0.50 | ms/batch 45.40 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30100/ 2126 batches | lr 0.50 | ms/batch 44.84 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30150/ 2126 batches | lr 0.50 | ms/batch 45.33 | loss  0.02 | ppl     1.02\n","| epoch   1 | 30200/ 2126 batches | lr 0.50 | ms/batch 44.64 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30250/ 2126 batches | lr 0.50 | ms/batch 44.60 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30300/ 2126 batches | lr 0.50 | ms/batch 44.64 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30350/ 2126 batches | lr 0.50 | ms/batch 44.90 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30400/ 2126 batches | lr 0.50 | ms/batch 44.75 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30450/ 2126 batches | lr 0.50 | ms/batch 45.40 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30500/ 2126 batches | lr 0.50 | ms/batch 44.96 | loss  0.02 | ppl     1.02\n","| epoch   1 | 30550/ 2126 batches | lr 0.50 | ms/batch 44.73 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30600/ 2126 batches | lr 0.50 | ms/batch 45.33 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30650/ 2126 batches | lr 0.50 | ms/batch 45.90 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30700/ 2126 batches | lr 0.50 | ms/batch 44.40 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30750/ 2126 batches | lr 0.50 | ms/batch 44.71 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30800/ 2126 batches | lr 0.50 | ms/batch 45.10 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30850/ 2126 batches | lr 0.50 | ms/batch 45.71 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30900/ 2126 batches | lr 0.50 | ms/batch 45.33 | loss  0.01 | ppl     1.01\n","| epoch   1 | 30950/ 2126 batches | lr 0.50 | ms/batch 45.36 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31000/ 2126 batches | lr 0.50 | ms/batch 45.34 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31050/ 2126 batches | lr 0.50 | ms/batch 45.02 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31100/ 2126 batches | lr 0.50 | ms/batch 44.81 | loss  0.02 | ppl     1.02\n","| epoch   1 | 31150/ 2126 batches | lr 0.50 | ms/batch 44.56 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31200/ 2126 batches | lr 0.50 | ms/batch 44.74 | loss  0.02 | ppl     1.02\n","| epoch   1 | 31250/ 2126 batches | lr 0.50 | ms/batch 45.12 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31300/ 2126 batches | lr 0.50 | ms/batch 45.27 | loss  0.02 | ppl     1.02\n","| epoch   1 | 31350/ 2126 batches | lr 0.50 | ms/batch 44.83 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31400/ 2126 batches | lr 0.50 | ms/batch 45.06 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31450/ 2126 batches | lr 0.50 | ms/batch 45.53 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31500/ 2126 batches | lr 0.50 | ms/batch 44.73 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31550/ 2126 batches | lr 0.50 | ms/batch 45.18 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31600/ 2126 batches | lr 0.50 | ms/batch 44.64 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31650/ 2126 batches | lr 0.50 | ms/batch 45.05 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31700/ 2126 batches | lr 0.50 | ms/batch 44.90 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31750/ 2126 batches | lr 0.50 | ms/batch 45.15 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31800/ 2126 batches | lr 0.50 | ms/batch 45.28 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31850/ 2126 batches | lr 0.50 | ms/batch 44.72 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31900/ 2126 batches | lr 0.50 | ms/batch 45.20 | loss  0.01 | ppl     1.01\n","| epoch   1 | 31950/ 2126 batches | lr 0.50 | ms/batch 44.70 | loss  0.01 | ppl     1.02\n","| epoch   1 | 32000/ 2126 batches | lr 0.50 | ms/batch 45.29 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32050/ 2126 batches | lr 0.50 | ms/batch 44.99 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32100/ 2126 batches | lr 0.50 | ms/batch 45.15 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32150/ 2126 batches | lr 0.50 | ms/batch 44.82 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32200/ 2126 batches | lr 0.50 | ms/batch 44.70 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32250/ 2126 batches | lr 0.50 | ms/batch 44.69 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32300/ 2126 batches | lr 0.50 | ms/batch 44.99 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32350/ 2126 batches | lr 0.50 | ms/batch 45.86 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32400/ 2126 batches | lr 0.50 | ms/batch 44.94 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32450/ 2126 batches | lr 0.50 | ms/batch 45.33 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32500/ 2126 batches | lr 0.50 | ms/batch 45.14 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32550/ 2126 batches | lr 0.50 | ms/batch 44.75 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32600/ 2126 batches | lr 0.50 | ms/batch 45.26 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32650/ 2126 batches | lr 0.50 | ms/batch 45.88 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32700/ 2126 batches | lr 0.50 | ms/batch 45.17 | loss  0.02 | ppl     1.02\n","| epoch   1 | 32750/ 2126 batches | lr 0.50 | ms/batch 45.40 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32800/ 2126 batches | lr 0.50 | ms/batch 45.65 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32850/ 2126 batches | lr 0.50 | ms/batch 45.18 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32900/ 2126 batches | lr 0.50 | ms/batch 44.65 | loss  0.01 | ppl     1.01\n","| epoch   1 | 32950/ 2126 batches | lr 0.50 | ms/batch 44.95 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33000/ 2126 batches | lr 0.50 | ms/batch 45.06 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33050/ 2126 batches | lr 0.50 | ms/batch 45.53 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33100/ 2126 batches | lr 0.50 | ms/batch 45.57 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33150/ 2126 batches | lr 0.50 | ms/batch 44.60 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33200/ 2126 batches | lr 0.50 | ms/batch 45.02 | loss  0.02 | ppl     1.02\n","| epoch   1 | 33250/ 2126 batches | lr 0.50 | ms/batch 44.88 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33300/ 2126 batches | lr 0.50 | ms/batch 44.74 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33350/ 2126 batches | lr 0.50 | ms/batch 45.06 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33400/ 2126 batches | lr 0.50 | ms/batch 44.81 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33450/ 2126 batches | lr 0.50 | ms/batch 44.64 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33500/ 2126 batches | lr 0.50 | ms/batch 45.29 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33550/ 2126 batches | lr 0.50 | ms/batch 44.65 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33600/ 2126 batches | lr 0.50 | ms/batch 44.62 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33650/ 2126 batches | lr 0.50 | ms/batch 45.00 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33700/ 2126 batches | lr 0.50 | ms/batch 45.19 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33750/ 2126 batches | lr 0.50 | ms/batch 44.88 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33800/ 2126 batches | lr 0.50 | ms/batch 44.82 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33850/ 2126 batches | lr 0.50 | ms/batch 44.66 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33900/ 2126 batches | lr 0.50 | ms/batch 44.81 | loss  0.01 | ppl     1.01\n","| epoch   1 | 33950/ 2126 batches | lr 0.50 | ms/batch 44.61 | loss  0.01 | ppl     1.01\n","| epoch   1 | 34000/ 2126 batches | lr 0.50 | ms/batch 44.88 | loss  0.01 | ppl     1.01\n","-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time: 1600.06s | valid loss  0.01 | valid ppl     1.01\n","-----------------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yz-pQVhpmt7U","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1592165515348,"user_tz":-120,"elapsed":567,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"5238e927-e880-4b60-82bc-3a82f1db9b57"},"source":["test = torch.tensor([1])\n","test = test.repeat(1, 16)\n","test.size()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 16])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"4Ml9rAVk4KUQ"},"source":["def generate(src,max_length): #src is has the [seq_l] dimensionality \n","  #get <pad> id\n","  pad_id = REP.vocab.stoi['<pad>'] #REP\n","  eos_id = REP.vocab.stoi['<eos>'] #REP\n","  sos_id = REP.vocab.stoi['<sos>'] #REP \n","  init_tgt = [sos_id]\n","  eos_counter = 0\n","  output_ner = []\n","  output_rep = [] \n","  for i in range(max_length):\n","    tgt_input = init_tgt + [pad_id]*(max_length-len(src))\n","    tgt_input = torch.tensor(tgt_input).to(device)\n","    #tgt_input = torch.tensor(list(REP.numericalize([tgt_input]))).to(device)\n","    output = best_model(src.view(1,len(src)),tgt_input.view(len(tgt_input),1))\n","    output_ner = output[-len(src):,0,:]\n","    output_rep = output[:-len(src),0,:]\n","    _,max_ind = torch.max(output_rep,1)\n","    rep = [REP.vocab.itos[i] for i in max_ind]\n","    init_tgt = init_tgt + [max_ind[i]]\n","    #if max_ind[i] == eos_id:\n","    #  eos_counter+=1\n","    #  if eos_counter ==2:\n","    #    break\n","    if max_ind[i] == eos_id:\n","      break\n","  _,max_ind = torch.max(output_ner,1)\n","  output_ner_text = [REP.vocab.itos[i] for i in max_ind]\n","  return ([REP.vocab.itos[i] for i in init_tgt],output_ner_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1WKmHUjqYgz","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1592165525204,"user_tz":-120,"elapsed":5500,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"d96f249a-70a3-489d-df51-1440fdbcde0a"},"source":["for batch in val_iter:\n","  data, targets_parse,targets_ner = get_batch(batch)\n","  print(data.size(),data[0,:].size())\n","  #print([TEXT.vocab.itos[i] for i in data[:,0]])\n","  print(\"Input:\",tokenizer.convert_ids_to_tokens(data[0,:]))\n","  #generate the representation\n","  output = generate(data[0,:],40)\n","  print(\"Output REP:\",output[0])\n","  print(\"Output NER:\",output[1])\n","  # generate the right answer\n","  print(\"Actual REP:\",[REP.vocab.itos[i] for i in batch.rep[:,0]])\n","  print(\"Actual NER:\",[REP.vocab.itos[i] for i in batch.ner[:,0]])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 17]) torch.Size([17])\n","Input: ['[CLS]', 'name', 'the', 'smallest', 'region', 'where', 'total', 'volume', 'is', 'equal', 'to', '268', '##8', '##31', '.', '64', '[SEP]']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["Output REP: ['<sos>', 'f_select', 'col_1', 'f_min', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'col_1', 'o', 'col_2', 'col_2', 'o', 'o', 'o', 'var_2', 'var_2', 'var_2', 'var_2', 'var_2', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_min', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'col_1', 'o', 'col_2', 'col_2', 'o', 'o', 'o', 'var_2', 'var_2', 'var_2', 'var_2', 'var_2', '<eos>']\n","torch.Size([1, 56]) torch.Size([56])\n","Input: ['[CLS]', 'name', 'the', 'largest', 'month', 'which', 'have', '8th', 'st', 'at', 'bran', '##nan', 'st', 'end', '_', 'station', '_', 'name', 'or', '1992', '.', '0', 'member', '_', 'birth', '_', 'year', 'or', 'member', '_', 'birth', '_', 'year', 'being', '1989', '.', '0', 'or', 'having', 'powell', 'st', 'bart', 'station', '(', 'market', 'st', 'at', '4th', 'st', ')', 'start', '_', 'station', '_', 'name', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'col_1', 'f_filter', 'df_name', 'f_or', 'f_eq', 'col_3', 'var_3', 'f_or', 'f_eq', 'col_4', 'var_4', 'f_or', 'f_eq', 'col_5', 'var_5', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'col_1', 'o', 'o', 'var_2', 'var_2', 'var_2', 'var_2', 'var_2', 'var_2', 'col_2', 'col_2', 'col_2', 'col_2', 'col_2', 'o', 'var_3', 'var_3', 'var_3', 'col_3', 'col_3', 'col_3', 'col_3', 'col_3', 'o', 'col_4', 'col_4', 'col_4', 'col_4', 'col_4', 'o', 'var_4', 'var_4', 'var_4', 'o', 'o', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'col_5', 'col_5', 'col_5', 'col_5', 'col_5', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'col_1', 'f_filter', 'df_name', 'f_or', 'f_eq', 'col_3', 'var_3', 'f_or', 'f_eq', 'col_4', 'var_4', 'f_or', 'f_eq', 'col_5', 'var_5', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'col_1', 'o', 'o', 'var_2', 'var_2', 'var_2', 'var_2', 'var_2', 'var_2', 'col_2', 'col_2', 'col_2', 'col_2', 'col_2', 'o', 'var_3', 'var_3', 'var_3', 'col_3', 'col_3', 'col_3', 'col_3', 'col_3', 'o', 'col_4', 'col_4', 'col_4', 'col_4', 'col_4', 'o', 'var_4', 'var_4', 'var_4', 'o', 'o', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', 'col_5', 'col_5', 'col_5', 'col_5', 'col_5', '<eos>']\n","torch.Size([1, 12]) torch.Size([12])\n","Input: ['[CLS]', 'what', 'is', 'the', 'smallest', 'predator', 'with', '6', 'class', '_', 'type', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_min', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'o', 'var_2', 'col_2', 'col_2', 'col_2', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_min', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'o', 'var_2', 'col_2', 'col_2', 'col_2', '<eos>']\n","torch.Size([1, 44]) torch.Size([44])\n","Input: ['[CLS]', 'what', 'is', 'the', 'largest', 'end', '_', 'station', '_', 'name', 'which', 'have', '1992', '.', '0', 'member', '_', 'birth', '_', 'year', 'or', 'member', '_', 'birth', '_', 'year', 'being', 'equal', '1995', '.', '0', 'and', 'member', '_', 'birth', '_', 'year', 'is', 'equal', 'to', '1994', '.', '0', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'col_1', 'f_filter', 'df_name', 'f_or', 'f_eq', 'col_3', 'var_3', 'f_or', 'f_eq', 'col_4', 'var_4', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'col_1', 'col_1', 'col_1', 'col_1', 'o', 'o', 'var_2', 'var_2', 'var_2', 'col_2', 'col_2', 'col_2', 'col_2', 'col_2', 'o', 'col_3', 'col_3', 'col_3', 'col_3', 'col_3', 'o', 'o', 'var_3', 'var_3', 'var_3', 'o', 'col_4', 'col_4', 'col_4', 'col_4', 'col_4', 'o', 'o', 'o', 'var_4', 'var_4', 'var_4', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'col_1', 'f_filter', 'df_name', 'f_or', 'f_eq', 'col_3', 'var_3', 'f_and', 'f_eq', 'col_4', 'var_4', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'col_1', 'col_1', 'col_1', 'col_1', 'o', 'o', 'var_2', 'var_2', 'var_2', 'col_2', 'col_2', 'col_2', 'col_2', 'col_2', 'o', 'col_3', 'col_3', 'col_3', 'col_3', 'col_3', 'o', 'o', 'var_3', 'var_3', 'var_3', 'o', 'col_4', 'col_4', 'col_4', 'col_4', 'col_4', 'o', 'o', 'o', 'var_4', 'var_4', 'var_4', '<eos>']\n","torch.Size([1, 9]) torch.Size([9])\n","Input: ['[CLS]', 'what', 'is', 'the', 'smallest', 'unnamed', ':', '0', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_min', 'col_1', 'df_name', '<eos>', 'df_name', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'col_1', 'col_1', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_min', 'col_1', 'df_name', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'col_1', 'col_1', '<eos>']\n","torch.Size([1, 30]) torch.Size([30])\n","Input: ['[CLS]', 'how', 'many', 'user', '_', 'type', 'have', '36', '##31', 'bike', '_', 'id', 'or', 'bike', '_', 'id', 'is', '189', 'and', 'start', '_', 'station', '_', 'id', 'being', 'equal', '146', '.', '0', '[SEP]']\n","Output REP: ['<sos>', 'f_length', 'f_filter', 'df_name', 'f_or', 'f_eq', 'col_3', 'var_3', 'f_or', 'f_eq', 'col_4', 'var_4', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'col_1', 'col_1', 'col_1', 'o', 'var_2', 'var_2', 'col_2', 'col_2', 'col_2', 'o', 'col_3', 'col_3', 'col_3', 'o', 'var_3', 'o', 'col_4', 'col_4', 'col_4', 'col_4', 'col_4', 'o', 'o', 'var_4', 'var_4', 'var_4', '<eos>']\n","Actual REP: ['<sos>', 'f_length', 'f_filter', 'df_name', 'f_or', 'f_eq', 'col_3', 'var_3', 'f_and', 'f_eq', 'col_4', 'var_4', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'col_1', 'col_1', 'col_1', 'o', 'var_2', 'var_2', 'col_2', 'col_2', 'col_2', 'o', 'col_3', 'col_3', 'col_3', 'o', 'var_3', 'o', 'col_4', 'col_4', 'col_4', 'col_4', 'col_4', 'o', 'o', 'var_4', 'var_4', 'var_4', '<eos>']\n","torch.Size([1, 10]) torch.Size([10])\n","Input: ['[CLS]', 'name', 'the', 'largest', 'breathe', '##s', 'with', '1', 'milk', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'col_1', 'col_1', 'o', 'var_2', 'col_2', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'col_1', 'col_1', 'o', 'var_2', 'col_2', '<eos>']\n","torch.Size([1, 10]) torch.Size([10])\n","Input: ['[CLS]', 'name', 'the', 'airborne', 'which', 'have', '1', 'breathe', '##s', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'col_1', 'o', 'o', 'var_2', 'col_2', 'col_2', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'col_1', 'o', 'o', 'var_2', 'col_2', 'col_2', '<eos>']\n","torch.Size([1, 11]) torch.Size([11])\n","Input: ['[CLS]', 'name', 'the', 'domestic', 'which', 'have', 'trout', 'animal', '_', 'name', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'col_1', 'o', 'o', 'var_2', 'col_2', 'col_2', 'col_2', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'col_1', 'o', 'o', 'var_2', 'col_2', 'col_2', 'col_2', '<eos>']\n","torch.Size([1, 15]) torch.Size([15])\n","Input: ['[CLS]', 'what', 'fire', '_', 'name', 'has', 'the', 'smallest', 'number', 'of', 'rim', 'fire', '_', 'name', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_min', 'count', 'f_count', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'col_1', 'col_1', 'col_1', 'o', 'o', 'o', 'o', 'o', 'var_2', 'col_2', 'col_2', 'col_2', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_min', 'count', 'f_count', 'col_1', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'col_1', 'col_1', 'col_1', 'o', 'o', 'o', 'o', 'o', 'var_2', 'col_2', 'col_2', 'col_2', '<eos>']\n","torch.Size([1, 13]) torch.Size([13])\n","Input: ['[CLS]', 'what', 'is', 'the', 'most', 'expensive', 'legs', 'with', '3', 'class', '_', 'type', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'price', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'o', 'o', 'col_1', 'o', 'var_2', 'col_2', 'col_2', 'col_2', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'price', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'o', 'o', 'col_1', 'o', 'var_2', 'col_2', 'col_2', 'col_2', '<eos>']\n","torch.Size([1, 8]) torch.Size([8])\n","Input: ['[CLS]', 'how', 'many', 'total', 'volume', 'is', 'there', '[SEP]']\n","Output REP: ['<sos>', 'f_length', 'f_select_unique', 'col_1', 'df_name', '<eos>', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'col_1', 'col_1', 'o', 'o', '<eos>']\n","Actual REP: ['<sos>', 'f_length', 'f_select_unique', 'col_1', 'df_name', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'col_1', 'col_1', 'o', 'o', '<eos>']\n","torch.Size([1, 54]) torch.Size([54])\n","Input: ['[CLS]', 'name', 'the', 'most', 'expensive', 'total', 'volume', 'with', '88', '##7', '##3', '.', '32', 'total', 'volume', 'or', '404', '##6', 'is', 'equal', 'to', '249', '##58', '.', '8', 'or', '47', '##70', 'being', 'equal', '288', '##5', '.', '0', 'and', '47', '##70', 'is', 'equal', 'to', '96', '##20', '.', '92', 'or', 'large', 'bags', 'is', '57', '##9', '##23', '.', '97', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'price', 'f_filter', 'df_name', 'f_or', 'f_eq', 'col_3', 'var_3', 'f_or', 'f_eq', 'col_4', 'var_4', 'f_or', 'f_eq', 'col_5', 'var_5', 'f_or', 'f_eq', 'col_6', 'var_6', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'col_1', 'o', 'var_2', 'var_2', 'var_2', 'var_2', 'var_2', 'col_2', 'col_2', 'o', 'col_3', 'col_3', 'o', 'o', 'o', 'var_3', 'var_3', 'var_3', 'var_3', 'o', 'col_4', 'col_4', 'o', 'o', 'var_4', 'var_4', 'var_4', 'var_4', 'o', 'col_5', 'col_5', 'o', 'o', 'o', 'var_5', 'var_5', 'var_5', 'var_5', 'o', 'col_6', 'col_6', 'o', 'var_6', 'var_6', 'var_6', 'var_6', 'var_6', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'price', 'f_filter', 'df_name', 'f_or', 'f_eq', 'col_3', 'var_3', 'f_or', 'f_eq', 'col_4', 'var_4', 'f_and', 'f_eq', 'col_5', 'var_5', 'f_or', 'f_eq', 'col_6', 'var_6', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'col_1', 'o', 'var_2', 'var_2', 'var_2', 'var_2', 'var_2', 'col_2', 'col_2', 'o', 'col_3', 'col_3', 'o', 'o', 'o', 'var_3', 'var_3', 'var_3', 'var_3', 'o', 'col_4', 'col_4', 'o', 'o', 'var_4', 'var_4', 'var_4', 'var_4', 'o', 'col_5', 'col_5', 'o', 'o', 'o', 'var_5', 'var_5', 'var_5', 'var_5', 'o', 'col_6', 'col_6', 'o', 'var_6', 'var_6', 'var_6', 'var_6', 'var_6', '<eos>']\n","torch.Size([1, 6]) torch.Size([6])\n","Input: ['[CLS]', 'what', 'are', 'the', 'airborne', '[SEP]']\n","Output REP: ['<sos>', 'f_select_unique', 'col_1', 'df_name', '<eos>', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'col_1', '<eos>']\n","Actual REP: ['<sos>', 'f_select_unique', 'col_1', 'df_name', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'col_1', '<eos>']\n","torch.Size([1, 36]) torch.Size([36])\n","Input: ['[CLS]', 'name', 'the', 'largest', 'xl', '##ar', '##ge', 'bags', 'where', 'year', 'is', '2016', 'and', 'having', 'conventional', 'type', 'or', 'date', 'is', 'equal', 'to', '2015', '-', '10', '-', '18', 'and', 'total', 'bags', 'being', '78', '##7', '##37', '.', '16', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'col_1', 'f_filter', 'df_name', 'f_or', 'f_eq', 'col_3', 'var_3', 'f_or', 'f_eq', 'col_4', 'var_4', 'f_or', 'f_eq', 'col_5', 'var_5', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'col_1', 'col_1', 'col_1', 'col_1', 'o', 'col_2', 'o', 'var_2', 'o', 'o', 'var_3', 'col_3', 'o', 'col_4', 'o', 'o', 'o', 'var_4', 'var_4', 'var_4', 'var_4', 'var_4', 'o', 'col_5', 'col_5', 'o', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'col_1', 'f_filter', 'df_name', 'f_and', 'f_eq', 'col_3', 'var_3', 'f_or', 'f_eq', 'col_4', 'var_4', 'f_and', 'f_eq', 'col_5', 'var_5', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'col_1', 'col_1', 'col_1', 'col_1', 'o', 'col_2', 'o', 'var_2', 'o', 'o', 'var_3', 'col_3', 'o', 'col_4', 'o', 'o', 'o', 'var_4', 'var_4', 'var_4', 'var_4', 'var_4', 'o', 'col_5', 'col_5', 'o', 'var_5', 'var_5', 'var_5', 'var_5', 'var_5', '<eos>']\n","torch.Size([1, 11]) torch.Size([11])\n","Input: ['[CLS]', 'name', 'the', 'cheap', '##est', 'acres', 'where', 'month', 'is', 'august', '[SEP]']\n","Output REP: ['<sos>', 'f_select', 'col_1', 'f_min', 'price', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'o', 'col_2', 'o', 'var_2', '<eos>']\n","Actual REP: ['<sos>', 'f_select', 'col_1', 'f_min', 'price', 'f_filter', 'df_name', 'f_eq', 'col_2', 'var_2', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'o', 'col_2', 'o', 'var_2', '<eos>']\n","torch.Size([1, 7]) torch.Size([7])\n","Input: ['[CLS]', 'name', 'the', 'feature', '_', '7', '[SEP]']\n","Output REP: ['<sos>', 'f_select_unique', 'col_1', 'df_name', '<eos>', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'col_1', 'col_1', 'col_1', '<eos>']\n","Actual REP: ['<sos>', 'f_select_unique', 'col_1', 'df_name', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'col_1', 'col_1', 'col_1', '<eos>']\n","torch.Size([1, 9]) torch.Size([9])\n","Input: ['[CLS]', 'count', 'different', 'engine', '_', 'has', '_', 'gas', '[SEP]']\n","Output REP: ['<sos>', 'f_length', 'f_select_unique', 'col_1', 'df_name', '<eos>', '<eos>']\n","Output NER: ['<sos>', 'o', 'o', 'col_1', 'col_1', 'col_1', 'col_1', 'col_1', '<eos>']\n","Actual REP: ['<sos>', 'f_length', 'f_select_unique', 'col_1', 'df_name', '<eos>']\n","Actual NER: ['<sos>', 'o', 'o', 'col_1', 'col_1', 'col_1', 'col_1', 'col_1', '<eos>']\n","torch.Size([1, 14]) torch.Size([14])\n","Input: ['[CLS]', 'what', 'is', 'the', 'most', 'expensive', 'large', 'bags', 'having', '6', 'unnamed', ':', '0', '[SEP]']\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-53e9ab93d0fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#generate the representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output REP:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output NER:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-c7b74f26160c>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(src, max_length)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtgt_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#tgt_input = torch.tensor(list(REP.numericalize([tgt_input]))).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0moutput_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moutput_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-fbe3a81d9104>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#print(src.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#with torch.no_grad(): #try to enable for NER purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# you need to permute the bert output because it has flipped batch/seq dimensions, otherwise transformer doesn't comsume it correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         )\n\u001b[1;32m    736\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 408\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \u001b[0mtens_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_overrides.py\u001b[0m in \u001b[0;36mhas_torch_function\u001b[0;34m(relevant_args)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0mimplementations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \"\"\"\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__torch_function__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_overridable_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"MIWBhFDUqlT_"},"source":["#test random input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oY39pdTXlkOX","colab":{"base_uri":"https://localhost:8080/","height":137},"executionInfo":{"status":"ok","timestamp":1592165688499,"user_tz":-120,"elapsed":997,"user":{"displayName":"Semantika labs","photoUrl":"","userId":"07619975397796211651"}},"outputId":"68c88c60-bd36-4eff-a774-3f4b66f5c97c"},"source":["test_input ='Name the most expensive animal_name with 124567 legs or 8 tentacles or fast speed'\n","#test_input ='Name the largest fat_legs with 1 predator'\n","fix_length = 59\n","#test_input ='Which priority has the most entries?'\n","#test_input ='What are unique priority'\n","src_input = tokenize(test_input)\n","src_input = ['[CLS]'] + src_input + ['[SEP]']\n","print('BERT tokenization:',src_input)\n","src_input = tokenizer.convert_tokens_to_ids(src_input)\n","#print('BERT decoded:',tokenizer.decode(src_input))\n","src_input = torch.tensor(src_input).to(device)\n","output = generate(src_input,fix_length)\n","print(\"REP:\",output[0])\n","print(\"NER:\",output[1])\n","ner_output = np.array(output[1])\n","entity_values = list(ner_output[ner_output!= 'o'][1:-1]) #cut sos and eos \n","entity_indexes = list(np.where(ner_output != 'o')[0][1:-1])\n","key_tokens = {}\n","for i,index in enumerate(list(entity_indexes)):\n","  token_list = key_tokens.get(entity_values[i],[])\n","  token_list.append(index)\n","  key_tokens[entity_values[i]] = token_list\n","for key in key_tokens:\n","  tokens = [src_input.cpu().numpy()[i] for i in key_tokens[key]]\n","  token_str = tokenizer.decode(tokens).replace(' _ ','_') #join words if underscore is between them\n","  key_tokens[key] = token_str \n","print(key_tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BERT tokenization: ['[CLS]', 'name', 'the', 'most', 'expensive', 'animal', '_', 'name', 'with', '124', '##56', '##7', 'legs', 'or', '8', 'tentacles', 'or', 'fast', 'speed', '[SEP]']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["REP: ['<sos>', 'f_select', 'col_1', 'f_max', 'price', 'f_filter', 'df_name', 'f_or', 'f_eq', 'col_3', 'var_3', 'f_or', 'f_eq', 'col_4', 'var_4', 'f_eq', 'col_2', 'var_2', '<eos>']\n","NER: ['<sos>', 'o', 'o', 'o', 'o', 'col_1', 'col_1', 'col_1', 'o', 'var_2', 'var_2', 'var_2', 'col_2', 'o', 'var_3', 'col_3', 'o', 'var_4', 'col_4', '<eos>']\n","{'col_1': 'animal_name', 'var_2': '124567', 'col_2': 'legs', 'var_3': '8', 'col_3': 'tentacles', 'var_4': 'fast', 'col_4': 'speed'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hfkAd2N5ZkEV"},"source":["torch.save(best_model.state_dict(), '/content/drive/My Drive/Models/model_14062020.state')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rX-3X6WIQuHa"},"source":["import pickle\n","pickle.dump(REP.vocab, open( \"/content/drive/My Drive/Models/vocab.pickle\", \"wb\" ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Acevi9u9RnZF"},"source":[""],"execution_count":null,"outputs":[]}]}